{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Constants"
      ],
      "metadata": {
        "id": "rtJZSgupvMTZ"
      },
      "id": "rtJZSgupvMTZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"4\">Imports </font>"
      ],
      "metadata": {
        "id": "UbwrfERBvbOG"
      },
      "id": "UbwrfERBvbOG"
    },
    {
      "metadata": {
        "jupyter": {
          "is_executing": true
        },
        "ExecuteTime": {
          "start_time": "2025-04-24T11:01:56.955022Z"
        },
        "id": "fbc121e30a2defb3"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch.nn as nn\n",
        "import zipfile\n",
        "import requests\n",
        "from glob import glob"
      ],
      "id": "fbc121e30a2defb3",
      "outputs": [],
      "execution_count": 54
    },
    {
      "metadata": {
        "id": "792fda7a90fc3204"
      },
      "cell_type": "markdown",
      "source": [
        "<font size=\"4\">Constants</font>"
      ],
      "id": "792fda7a90fc3204"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-23T12:57:22.636614Z",
          "start_time": "2025-04-23T12:57:22.628140Z"
        },
        "id": "4ab05b4b7ab2bc28"
      },
      "cell_type": "code",
      "source": [
        "current_working_directory = os.getcwd()\n",
        "DATA_BASE_DIRECTORY: str = os.path.join(current_working_directory, 'data')\n",
        "TRAINING_SET_URL='https://web.archive.org/web/20241214060505/https://vis-www.cs.umass.edu/lfw/pairsDevTrain.txt'\n",
        "TEST_SET_URL='https://web.archive.org/web/20241214070147/https://vis-www.cs.umass.edu/lfw/pairsDevTest.txt#expand'"
      ],
      "id": "4ab05b4b7ab2bc28",
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Acquiring & Handling Data"
      ],
      "metadata": {
        "id": "-2XQCQ27vUIF"
      },
      "id": "-2XQCQ27vUIF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"6\">Acquiring_Data</font>"
      ],
      "metadata": {
        "id": "f6UkO1B1wS3I"
      },
      "id": "f6UkO1B1wS3I"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gdown\n",
        "\n",
        "def download_images_from_drive(file_id: str, zip_path: str) -> str:\n",
        "  \"\"\"\n",
        "  Downloads images from drive and return the path to the extracted folder, but 1\n",
        "  level down assuming the structure of the directories are known in advance.\n",
        "  \"\"\"\n",
        "  file_location: str = os.path.join(DATA_BASE_DIRECTORY, 'lfw2', 'lfw2')\n",
        "  if os.path.exists(file_location):\n",
        "    print(f\"Dataset already downloaded to {file_location}\")\n",
        "    return file_location\n",
        "  !gdown {file_id} -O {zip_path}\n",
        "\n",
        "  os.makedirs(DATA_BASE_DIRECTORY, exist_ok=True)\n",
        "  with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(DATA_BASE_DIRECTORY)\n",
        "  !rm {zip_path}\n",
        "  print(f\"Dataset extracted to {DATA_BASE_DIRECTORY}\")\n",
        "  return file_location\n",
        "\n",
        "updated_dir_location: str = download_images_from_drive(\n",
        "    file_id=\"1p1wjaqpTh_5RHfJu4vUh8JJCdKwYMHCp\",\n",
        "    zip_path=\"dataset.zip\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c2tOa7RwRsW",
        "outputId": "8160c2c6-5862-4e63-abe2-c0b3f7dd733b"
      },
      "id": "_c2tOa7RwRsW",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already downloaded to /content/data/lfw2/lfw2\n",
            "/content/data/lfw2/lfw2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"4\">Loading file paths to memory</font>"
      ],
      "metadata": {
        "id": "CzrceAmpqKWD"
      },
      "id": "CzrceAmpqKWD"
    },
    {
      "cell_type": "code",
      "source": [
        "def loads_files_paths_to_memory(base_directory: str, image_format: str = '.jpg') -> None:\n",
        "    images: dict[str, dict[str, str]] = dict()\n",
        "    images_loaded: int = 0\n",
        "    for root, subdirs, files in os.walk(base_directory):\n",
        "        if root == base_directory:\n",
        "            continue\n",
        "        person_name: str = root.split(os.sep)[-1]\n",
        "        if person_name not in images:\n",
        "            images[person_name] = dict()\n",
        "        for file in files:\n",
        "            if not file.endswith(image_format):\n",
        "                raise Warning(f\"File {file} is not a {image_format} file. Continuing...\")\n",
        "                continue\n",
        "            stripped_image: str = file.rstrip(image_format) # File without ending\n",
        "            image_index: int = int(stripped_image.split('_')[-1])\n",
        "            if image_index in images[person_name]:\n",
        "                 raise ValueError(f\"Index: {image_index} collision for: {person_name}\")\n",
        "            images[person_name][image_index] = os.path.join(root, file)\n",
        "            images_loaded += 1\n",
        "    if len(images) < 1:\n",
        "        raise ValueError(f\"No images were found in {base_directory}, aborting...\")\n",
        "    print(f\"People scanned: {len(images)}\")\n",
        "    print(f\"Images loaded: {images_loaded}\")\n",
        "    return images"
      ],
      "metadata": {
        "id": "qiR7LxGoqJt3"
      },
      "id": "qiR7LxGoqJt3",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"4\">Organizing According to example-validation-train</font>"
      ],
      "metadata": {
        "id": "dhLUMropr8or"
      },
      "id": "dhLUMropr8or"
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_train_test_txt(url_to_use: str):\n",
        "    url_response = requests.get(url_to_use)\n",
        "    if url_response.status_code == 200:\n",
        "        text_content = url_response.text\n",
        "    else:\n",
        "        raise ValueError(\"Invalid URL\")\n",
        "    ret_text: list[str] = text_content.split('\\n')\n",
        "    positive_examples: list[tuple[str, int, int]] = list()\n",
        "    negative_examples: list[tuple[tuple[str, int], tuple[str, int]]] = list()\n",
        "    for text in ret_text:\n",
        "        separated_by_tabs: list[str] = text.split('\\t')\n",
        "        if len(separated_by_tabs) < 3:\n",
        "            # This is the number in the beginning\n",
        "            continue\n",
        "        if len(separated_by_tabs) == 3:\n",
        "            # This is a positive example (2 Pictures of the same person)\n",
        "            person = separated_by_tabs[0]\n",
        "            first_image_index = int(separated_by_tabs[1])\n",
        "            second_image_index = int(separated_by_tabs[2])\n",
        "            positive_examples.append((person, first_image_index, second_image_index))\n",
        "        if len(separated_by_tabs) == 4:\n",
        "            first_person = separated_by_tabs[0]\n",
        "            first_person_image_index = int(separated_by_tabs[1])\n",
        "            second_person = separated_by_tabs[2]\n",
        "            second_person_image_index = int(separated_by_tabs[3])\n",
        "            negative_examples.append(\n",
        "                                        (\n",
        "                                             (first_person, first_person_image_index),\n",
        "                                             (second_person, second_person_image_index)\n",
        "                                        )\n",
        "                                     )\n",
        "    return positive_examples, negative_examples\n",
        "\n",
        "  training_positive_examples, training_negative_examples = parse_train_test_txt(TRAINING_SET_URL)\n",
        "  test_positive_examples, test_negative_examples = parse_train_test_txt(TEST_SET_URL)"
      ],
      "metadata": {
        "id": "RkEBUAZyvBqR"
      },
      "id": "RkEBUAZyvBqR",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e8da3580c132e0a5"
      },
      "cell_type": "markdown",
      "source": [
        "<font size=\"6\">Handling_Data</font>"
      ],
      "id": "e8da3580c132e0a5"
    },
    {
      "metadata": {
        "id": "c1528f83f48d4ec8"
      },
      "cell_type": "markdown",
      "source": [
        "<font size=\"6\">Creating Network</font>"
      ],
      "id": "c1528f83f48d4ec8"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-24T11:01:51.240312Z",
          "start_time": "2025-04-24T11:01:50.832893Z"
        },
        "id": "bcd78c34b2aa484d"
      },
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=10),  # TODO Change the in channels to support more colors.\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=7),  # TODO Change the in channels to support more colors.\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=4),  # TODO Change the in channels to support more colors.\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4),  # TODO Change the in channels to support more colors.\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Flatten(),                         # -> [256*6*6 = 9216]\n",
        "            nn.Linear(256*6*6, 4096),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(4096, 1),\n",
        "            nn.Sigmoid()  # similarity score\n",
        "        )\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        out1 = self.forward_once(input1)\n",
        "        out2 = self.forward_once(input2)\n",
        "        # L1 distance\n",
        "        diff = torch.abs(out1 - out2)\n",
        "        similarity_score = self.fc2(diff)\n",
        "        return similarity_score\n",
        "\n",
        "\n",
        "\n",
        "# asd = ConvNet()"
      ],
      "id": "bcd78c34b2aa484d",
      "outputs": [],
      "execution_count": 5
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}