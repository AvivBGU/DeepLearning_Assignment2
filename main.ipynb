{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvivBGU/DeepLearning_Assignment2/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODOS:\n",
        "1. Create a proper training setup and embed the stuff implemented in the data loaded to the relevant functions.\n",
        "2. Make sure that loss and iterations, as well as general data is printed.\n",
        "3. Have a testing setup for an arbitrary model.\n",
        "4. Create a model factory to properly balance different parameters of the network.\n",
        "5. Make sure that the testing setup allows the display of images to have visual verifacation."
      ],
      "metadata": {
        "id": "D39Befbt9pDh"
      },
      "id": "D39Befbt9pDh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Constants"
      ],
      "metadata": {
        "id": "rtJZSgupvMTZ"
      },
      "id": "rtJZSgupvMTZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"4\">Imports </font>"
      ],
      "metadata": {
        "id": "UbwrfERBvbOG"
      },
      "id": "UbwrfERBvbOG"
    },
    {
      "metadata": {
        "jupyter": {
          "is_executing": true
        },
        "ExecuteTime": {
          "start_time": "2025-04-24T11:01:56.955022Z"
        },
        "id": "fbc121e30a2defb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b76114-3fde-417e-80d3-98242e9668dc"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import zipfile\n",
        "import requests\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "print(\"Using torch\", torch.__version__)"
      ],
      "id": "fbc121e30a2defb3",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using torch 2.6.0+cu124\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "792fda7a90fc3204"
      },
      "cell_type": "markdown",
      "source": [
        "<font size=\"4\">Constants</font>"
      ],
      "id": "792fda7a90fc3204"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-23T12:57:22.636614Z",
          "start_time": "2025-04-23T12:57:22.628140Z"
        },
        "id": "4ab05b4b7ab2bc28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f119e0d0-dd08-471e-d908-c78eefec632d"
      },
      "cell_type": "code",
      "source": [
        "current_working_directory = os.getcwd()\n",
        "DATA_BASE_DIRECTORY: str = os.path.join(current_working_directory, 'data')\n",
        "TRAINING_SET_URL='https://web.archive.org/web/20241214060505/https://vis-www.cs.umass.edu/lfw/pairsDevTrain.txt'\n",
        "TEST_SET_URL='https://web.archive.org/web/20241214070147/https://vis-www.cs.umass.edu/lfw/pairsDevTest.txt#expand'\n",
        "MAX_PIXEL_VALUE: float = 255.0\n",
        "IMAGE_SIZE: tuple[int, int] = (250, 250)\n",
        "IMAGE_MODE: str = 'L' # If the image is greyscale\n",
        "DEVICE_TO_USE: str = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {DEVICE_TO_USE}')"
      ],
      "id": "4ab05b4b7ab2bc28",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Acquiring & Handling Data"
      ],
      "metadata": {
        "id": "-2XQCQ27vUIF"
      },
      "id": "-2XQCQ27vUIF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"6\">Acquiring_Data</font>"
      ],
      "metadata": {
        "id": "f6UkO1B1wS3I"
      },
      "id": "f6UkO1B1wS3I"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gdown\n",
        "\n",
        "def download_images_from_drive(file_id: str, zip_path: str) -> str:\n",
        "  \"\"\"\n",
        "  Downloads images from drive and return the path to the extracted folder, but 1\n",
        "  level down assuming the structure of the directories are known in advance.\n",
        "  \"\"\"\n",
        "  file_location: str = os.path.join(DATA_BASE_DIRECTORY, 'lfw2', 'lfw2')\n",
        "  if os.path.exists(file_location):\n",
        "    print(f\"Dataset already downloaded to {file_location}\")\n",
        "    return file_location\n",
        "  !gdown {file_id} -O {zip_path}\n",
        "\n",
        "  os.makedirs(DATA_BASE_DIRECTORY, exist_ok=True)\n",
        "  with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(DATA_BASE_DIRECTORY)\n",
        "  !rm {zip_path}\n",
        "  print(f\"Dataset extracted to {DATA_BASE_DIRECTORY}\")\n",
        "  return file_location\n",
        "\n",
        "updated_dir_location: str = download_images_from_drive(\n",
        "    file_id=\"1p1wjaqpTh_5RHfJu4vUh8JJCdKwYMHCp\",\n",
        "    zip_path=\"dataset.zip\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c2tOa7RwRsW",
        "outputId": "daefd489-5ad4-4054-b784-7b52be6d24e4"
      },
      "id": "_c2tOa7RwRsW",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1p1wjaqpTh_5RHfJu4vUh8JJCdKwYMHCp\n",
            "From (redirected): https://drive.google.com/uc?id=1p1wjaqpTh_5RHfJu4vUh8JJCdKwYMHCp&confirm=t&uuid=8ffcedf2-204e-4fdb-b081-94173f687742\n",
            "To: /content/dataset.zip\n",
            "100% 104M/104M [00:01<00:00, 54.4MB/s] \n",
            "Dataset extracted to /content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"4\">Preprocessing function</font>"
      ],
      "metadata": {
        "id": "69YLzhf59ERr"
      },
      "id": "69YLzhf59ERr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"4\">Loading file paths to memory</font>"
      ],
      "metadata": {
        "id": "CzrceAmpqKWD"
      },
      "id": "CzrceAmpqKWD"
    },
    {
      "cell_type": "code",
      "source": [
        "def loads_files_paths_to_memory(base_directory: str, image_format: str = '.jpg') -> None:\n",
        "    images: dict[str, dict[int, str]] = dict()\n",
        "    images_loaded: int = 0\n",
        "    for root, subdirs, files in os.walk(base_directory):\n",
        "        if root == base_directory:\n",
        "            continue\n",
        "        person_name: str = root.split(os.sep)[-1]\n",
        "        if person_name not in images:\n",
        "            images[person_name] = dict()\n",
        "        for file in files:\n",
        "            if not file.endswith(image_format):\n",
        "                raise Warning(f\"File {file} is not a {image_format} file. Continuing...\")\n",
        "                continue\n",
        "            stripped_image: str = file.rstrip(image_format) # File without ending\n",
        "            image_index: int = int(stripped_image.split('_')[-1])\n",
        "            if image_index in images[person_name]:\n",
        "                 raise ValueError(f\"Index: {image_index} collision for: {person_name}\")\n",
        "            images[person_name][image_index] = os.path.join(root, file)\n",
        "            images_loaded += 1\n",
        "    if len(images) < 1:\n",
        "        raise ValueError(f\"No images were found in {base_directory}, aborting...\")\n",
        "    print(f\"People scanned: {len(images)}\")\n",
        "    print(f\"Images loaded: {images_loaded}\")\n",
        "    return images\n",
        "\n",
        "loaded_images: dict[str, dict[str, str]] = loads_files_paths_to_memory(updated_dir_location)"
      ],
      "metadata": {
        "id": "qiR7LxGoqJt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aa62bc6-429d-4795-929c-5bf02ef5402d"
      },
      "id": "qiR7LxGoqJt3",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "People scanned: 5749\n",
            "Images loaded: 13233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"4\">Organizing According to train-test</font>"
      ],
      "metadata": {
        "id": "dhLUMropr8or"
      },
      "id": "dhLUMropr8or"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get train-test division and parse it."
      ],
      "metadata": {
        "id": "NVTbWWRE6Vo0"
      },
      "id": "NVTbWWRE6Vo0"
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_train_test_txt(url_to_use: str):\n",
        "    url_response = requests.get(url_to_use)\n",
        "    if url_response.status_code == 200:\n",
        "        text_content = url_response.text\n",
        "    else:\n",
        "        raise ValueError(\"Invalid URL\")\n",
        "    ret_text: list[str] = text_content.split('\\n')\n",
        "    examples: list[tuple[tuple[str, int], tuple[str, int], bool]] = list()\n",
        "    for text in ret_text:\n",
        "        separated_by_tabs: list[str] = text.split('\\t')\n",
        "        if len(separated_by_tabs) < 3:\n",
        "            # This is the number in the beginning\n",
        "            continue\n",
        "        if len(separated_by_tabs) == 3:\n",
        "            # This is a positive example (2 Pictures of the same person)\n",
        "            person = separated_by_tabs[0]\n",
        "            first_image_index = int(separated_by_tabs[1])\n",
        "            second_image_index = int(separated_by_tabs[2])\n",
        "            examples.append(\n",
        "                                        (\n",
        "                                             (person, first_image_index),\n",
        "                                             (person, second_image_index),\n",
        "                                             1\n",
        "                                        )\n",
        "                                     )\n",
        "        if len(separated_by_tabs) == 4:\n",
        "            first_person = separated_by_tabs[0]\n",
        "            first_person_image_index = int(separated_by_tabs[1])\n",
        "            second_person = separated_by_tabs[2]\n",
        "            second_person_image_index = int(separated_by_tabs[3])\n",
        "            examples.append(\n",
        "                                        (\n",
        "                                             (first_person, first_person_image_index),\n",
        "                                             (second_person, second_person_image_index),\n",
        "                                             0\n",
        "                                        )\n",
        "                                     )\n",
        "    return examples\n",
        "\n",
        "training_examples = parse_train_test_txt(TRAINING_SET_URL)\n",
        "test_examples = parse_train_test_txt(TEST_SET_URL)"
      ],
      "metadata": {
        "id": "RkEBUAZyvBqR"
      },
      "id": "RkEBUAZyvBqR",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(images_file_paths_dict: dict[str, dict[int, str]],\n",
        "                examples_list: list[tuple[tuple[str, int], tuple[str, int], bool]]) -> list[tuple[Image.Image, Image.Image]]:\n",
        "  \"\"\"\n",
        "  Loads the images given to memory in the following format:\n",
        "  Returns 2 lists:\n",
        "  list[loaded_image, loaded_image], list[is_same]\n",
        "  \"\"\"\n",
        "  data_to_ret: list = list()\n",
        "  labels_to_ret: list[bool] = list() # Returned labels, true if same person, false otherwise.\n",
        "  for example in examples_list:\n",
        "    first_person, first_image_index = example[0]\n",
        "    second_person, second_image_index = example[1]\n",
        "    is_same = example[2]\n",
        "    first_image_path = images_file_paths_dict[first_person][first_image_index]\n",
        "    second_image_path = images_file_paths_dict[second_person][second_image_index]\n",
        "    first_image = Image.open(first_image_path)\n",
        "    second_image = Image.open(second_image_path)\n",
        "    if (first_image.mode != IMAGE_MODE) or (second_image.mode != IMAGE_MODE):\n",
        "        raise ValueError(\"Images have different modes.\")\n",
        "    if (first_image.size != IMAGE_SIZE) or (second_image.size != IMAGE_SIZE):\n",
        "        raise ValueError(\"Images have different sizes.\")\n",
        "    data_to_ret.append((first_image, second_image))\n",
        "    labels_to_ret.append(is_same)\n",
        "  return data_to_ret, labels_to_ret\n",
        "\n",
        "training_data, training_labels = load_images(loaded_images, training_examples)\n",
        "test_data, test_labels = load_images(loaded_images, test_examples)"
      ],
      "metadata": {
        "id": "BqvZnIBp6gsl"
      },
      "id": "BqvZnIBp6gsl",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_images_to_array(image_tuple_list: list[tuple[Image.Image, Image.Image]]) -> list[np.ndarray]:\n",
        "  returned_list: list[np.ndarray] = list()\n",
        "  for first_image, second_image in image_tuple_list:\n",
        "    normalized_first_image = np.array(first_image) / MAX_PIXEL_VALUE\n",
        "    normalized_second_image = np.array(second_image) / MAX_PIXEL_VALUE\n",
        "    first_image_array = np.array(normalized_first_image)\n",
        "    second_image_array = np.array(normalized_second_image)\n",
        "    returned_list.append((first_image_array, second_image_array))\n",
        "  return returned_list\n",
        "\n",
        "arrayed_training_data = convert_images_to_array(training_data)\n",
        "arrayed_test_data = convert_images_to_array(test_data)"
      ],
      "metadata": {
        "id": "QWyz_nkdqxT4"
      },
      "id": "QWyz_nkdqxT4",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network definition and stuff"
      ],
      "metadata": {
        "id": "1LqVSt-FtCJJ"
      },
      "id": "1LqVSt-FtCJJ"
    },
    {
      "metadata": {
        "id": "c1528f83f48d4ec8"
      },
      "cell_type": "markdown",
      "source": [
        "<font size=\"6\">Creating Network</font>"
      ],
      "id": "c1528f83f48d4ec8"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-24T11:01:51.240312Z",
          "start_time": "2025-04-24T11:01:50.832893Z"
        },
        "id": "bcd78c34b2aa484d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "9f242c80-9755-4973-cb0d-0b50263a4697"
      },
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=10),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=7),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=4),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "        dummy_input = torch.zeros(1, 1, 250, 250)\n",
        "        dummy_output = self.cnn(dummy_input)\n",
        "        flattened_size = dummy_output.view(1, -1).size(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(flattened_size, 4096),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.final_fc = nn.Linear(4096, 1)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight, mean=0.0, std=1e-2)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.normal_(m.bias, mean=0.5, std=1e-2)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, mean=0.0, std=2e-1)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.normal_(m.bias, mean=0.5, std=1e-2)\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        output1 = self.forward_once(input1)\n",
        "        output2 = self.forward_once(input2)\n",
        "\n",
        "        l1_distance = torch.abs(output1 - output2)\n",
        "        output = self.final_fc(l1_distance)\n",
        "        return output\n",
        "\n",
        "class RegularizedBinaryCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, model, lambda_reg=1e-4):\n",
        "        super(RegularizedBinaryCrossEntropyLoss, self).__init__()\n",
        "        self.model = model\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        loss = self.bce(outputs, targets)\n",
        "        reg_loss = 0.0\n",
        "        for param in self.model.parameters():\n",
        "            reg_loss += torch.sum(param ** 2)\n",
        "        total_loss = loss + self.lambda_reg * reg_loss\n",
        "        return total_loss\n",
        "\n",
        "class CustomSGDWithMomentum(torch.optim.Optimizer):\n",
        "    def __init__(self, params, lr=0.01, momentum=0.9, weight_decay=1e-4):\n",
        "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "        super(CustomSGDWithMomentum, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                if group['weight_decay'] != 0:\n",
        "                    d_p.add_(2 * group['weight_decay'], p.data)\n",
        "                if 'momentum_buffer' not in self.state[p]:\n",
        "                    buf = self.state[p]['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                else:\n",
        "                    buf = self.state[p]['momentum_buffer']\n",
        "                    buf.mul_(group['momentum']).add_(d_p, alpha=-group['lr'])\n",
        "                p.data.add_(buf)\n",
        "\n",
        "        return loss\n",
        "\n",
        "class SiameseDataset(data.Dataset):\n",
        "    def __init__(self, image_pairs, labels):\n",
        "        self.image_pairs = image_pairs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1, img2 = self.image_pairs[idx]\n",
        "        img1 = torch.tensor(np.array(img1), dtype=torch.float32).unsqueeze(0)  # [1, H, W]\n",
        "        img2 = torch.tensor(np.array(img2), dtype=torch.float32).unsqueeze(0)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return img1, img2, label\n",
        "\n",
        "# Example training and validation data (should be prepared externally)\n",
        "# training_data = [(Image1, Image2), ...]\n",
        "# labels = [1, 0, 1, ...]\n",
        "# val_data = [(Image1, Image2), ...]\n",
        "# val_labels = [1, 0, 1, ...]\n",
        "\n",
        "# train_dataset = SiameseDataset(training_data, training_labels)\n",
        "# # val_dataset = SiameseDataset(val_data, val_labels)\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True)\n",
        "# # val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False, pin_memory=True)\n",
        "# model = SiameseNetwork().to(DEVICE_TO_USE)\n",
        "# criterion = RegularizedBinaryCrossEntropyLoss(model, lambda_reg=1e-4)\n",
        "# optimizer = CustomSGDWithMomentum(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "# num_epochs = 10\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     for input1, input2, targets in train_loader:\n",
        "#         input1, input2, targets = input1.to(DEVICE_TO_USE, non_blocking=True), input2.to(DEVICE_TO_USE, non_blocking=True), targets.unsqueeze(1).to(DEVICE_TO_USE, non_blocking=True)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         output = model(input1, input2)\n",
        "#         loss = criterion(output, targets)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     model.eval()\n",
        "#     torch.cuda.empty_cache()\n",
        "    # val_loss = 0.0\n",
        "    # with torch.no_grad():\n",
        "    #     for val_input1, val_input2, val_targets in val_loader:\n",
        "    #         val_input1, val_input2, val_targets = val_input1.to(DEVICE_TO_USE, non_blocking=True), val_input2.to(DEVICE_TO_USE, non_blocking=True), val_targets.unsqueeze(1).to(device, non_blocking=True)\n",
        "    #         val_output = model(val_input1, val_input2)\n",
        "    #         batch_loss = criterion(val_output, val_targets)\n",
        "    #         val_loss += batch_loss.item()\n",
        "    # val_loss /= len(val_loader)"
      ],
      "id": "bcd78c34b2aa484d",
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-889edee19df0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSiameseNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSiameseNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "test_dataset = SiameseDataset(test_data, test_labels)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False, pin_memory=True)\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for test_input1, test_input2, test_targets in test_loader:\n",
        "        test_input1, test_input2, test_targets = test_input1.to(DEVICE_TO_USE, non_blocking=True), test_input2.to(DEVICE_TO_USE, non_blocking=True), test_targets.unsqueeze(1).to(DEVICE_TO_USE, non_blocking=True)\n",
        "        test_output = model(test_input1, test_input2)\n",
        "        batch_loss = criterion(test_output, test_targets)\n",
        "        test_loss += batch_loss.item()\n",
        "        predictions = torch.sigmoid(test_output) > 0.5\n",
        "        correct = (predictions.float() == test_targets).sum().item()\n",
        "        correct_predictions += correct\n",
        "        total_predictions += test_targets.size(0)\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "accuracy = correct_predictions / total_predictions\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "XbgFaAvn9OP5",
        "outputId": "96a1ed60-6e08-43bd-c433-00f875fa5fbc"
      },
      "id": "XbgFaAvn9OP5",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'SiameseDataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-83606c9fa1bd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSiameseDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SiameseDataset' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}