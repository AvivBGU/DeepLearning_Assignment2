{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvivBGU/DeepLearning_Assignment2/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODOS:\n",
        "1. Given the training-test split, take the data and separated using the indexes acquired.\n",
        "2. Add preprocessing to the images.\n",
        "3. A bigass list, like in keras, will do.\n",
        "4. The list will contain [(image, image, is_same: bool)]\n"
      ],
      "metadata": {
        "id": "D39Befbt9pDh"
      },
      "id": "D39Befbt9pDh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Constants"
      ],
      "metadata": {
        "id": "rtJZSgupvMTZ"
      },
      "id": "rtJZSgupvMTZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"4\">Imports </font>"
      ],
      "metadata": {
        "id": "UbwrfERBvbOG"
      },
      "id": "UbwrfERBvbOG"
    },
    {
      "metadata": {
        "jupyter": {
          "is_executing": true
        },
        "ExecuteTime": {
          "start_time": "2025-04-24T11:01:56.955022Z"
        },
        "id": "fbc121e30a2defb3"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch.nn as nn\n",
        "import zipfile\n",
        "import requests\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "id": "fbc121e30a2defb3",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "792fda7a90fc3204"
      },
      "cell_type": "markdown",
      "source": [
        "<font size=\"4\">Constants</font>"
      ],
      "id": "792fda7a90fc3204"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-23T12:57:22.636614Z",
          "start_time": "2025-04-23T12:57:22.628140Z"
        },
        "id": "4ab05b4b7ab2bc28"
      },
      "cell_type": "code",
      "source": [
        "current_working_directory = os.getcwd()\n",
        "DATA_BASE_DIRECTORY: str = os.path.join(current_working_directory, 'data')\n",
        "TRAINING_SET_URL='https://web.archive.org/web/20241214060505/https://vis-www.cs.umass.edu/lfw/pairsDevTrain.txt'\n",
        "TEST_SET_URL='https://web.archive.org/web/20241214070147/https://vis-www.cs.umass.edu/lfw/pairsDevTest.txt#expand'\n",
        "MAX_PIXEL_VALUE: float = 255.0\n",
        "IMAGE_SIZE: tuple[int, int] = (250, 250)\n",
        "IMAGE_MODE: str = 'L' # If the image is greyscale"
      ],
      "id": "4ab05b4b7ab2bc28",
      "outputs": [],
      "execution_count": 35
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Acquiring & Handling Data"
      ],
      "metadata": {
        "id": "-2XQCQ27vUIF"
      },
      "id": "-2XQCQ27vUIF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"6\">Acquiring_Data</font>"
      ],
      "metadata": {
        "id": "f6UkO1B1wS3I"
      },
      "id": "f6UkO1B1wS3I"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gdown\n",
        "\n",
        "def download_images_from_drive(file_id: str, zip_path: str) -> str:\n",
        "  \"\"\"\n",
        "  Downloads images from drive and return the path to the extracted folder, but 1\n",
        "  level down assuming the structure of the directories are known in advance.\n",
        "  \"\"\"\n",
        "  file_location: str = os.path.join(DATA_BASE_DIRECTORY, 'lfw2', 'lfw2')\n",
        "  if os.path.exists(file_location):\n",
        "    print(f\"Dataset already downloaded to {file_location}\")\n",
        "    return file_location\n",
        "  !gdown {file_id} -O {zip_path}\n",
        "\n",
        "  os.makedirs(DATA_BASE_DIRECTORY, exist_ok=True)\n",
        "  with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(DATA_BASE_DIRECTORY)\n",
        "  !rm {zip_path}\n",
        "  print(f\"Dataset extracted to {DATA_BASE_DIRECTORY}\")\n",
        "  return file_location\n",
        "\n",
        "updated_dir_location: str = download_images_from_drive(\n",
        "    file_id=\"1p1wjaqpTh_5RHfJu4vUh8JJCdKwYMHCp\",\n",
        "    zip_path=\"dataset.zip\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c2tOa7RwRsW",
        "outputId": "46a0e954-cf3d-4f28-ed00-233cb1387797"
      },
      "id": "_c2tOa7RwRsW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1p1wjaqpTh_5RHfJu4vUh8JJCdKwYMHCp\n",
            "From (redirected): https://drive.google.com/uc?id=1p1wjaqpTh_5RHfJu4vUh8JJCdKwYMHCp&confirm=t&uuid=ed121b34-6d68-4c53-8141-25eb910aab19\n",
            "To: /content/dataset.zip\n",
            "100% 104M/104M [00:00<00:00, 157MB/s]\n",
            "Dataset extracted to /content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"4\">Preprocessing function</font>"
      ],
      "metadata": {
        "id": "69YLzhf59ERr"
      },
      "id": "69YLzhf59ERr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"4\">Loading file paths to memory</font>"
      ],
      "metadata": {
        "id": "CzrceAmpqKWD"
      },
      "id": "CzrceAmpqKWD"
    },
    {
      "cell_type": "code",
      "source": [
        "def loads_files_paths_to_memory(base_directory: str, image_format: str = '.jpg') -> None:\n",
        "    images: dict[str, dict[int, str]] = dict()\n",
        "    images_loaded: int = 0\n",
        "    for root, subdirs, files in os.walk(base_directory):\n",
        "        if root == base_directory:\n",
        "            continue\n",
        "        person_name: str = root.split(os.sep)[-1]\n",
        "        if person_name not in images:\n",
        "            images[person_name] = dict()\n",
        "        for file in files:\n",
        "            if not file.endswith(image_format):\n",
        "                raise Warning(f\"File {file} is not a {image_format} file. Continuing...\")\n",
        "                continue\n",
        "            stripped_image: str = file.rstrip(image_format) # File without ending\n",
        "            image_index: int = int(stripped_image.split('_')[-1])\n",
        "            if image_index in images[person_name]:\n",
        "                 raise ValueError(f\"Index: {image_index} collision for: {person_name}\")\n",
        "            images[person_name][image_index] = os.path.join(root, file)\n",
        "            images_loaded += 1\n",
        "    if len(images) < 1:\n",
        "        raise ValueError(f\"No images were found in {base_directory}, aborting...\")\n",
        "    print(f\"People scanned: {len(images)}\")\n",
        "    print(f\"Images loaded: {images_loaded}\")\n",
        "    return images\n",
        "\n",
        "loaded_images: dict[str, dict[str, str]] = loads_files_paths_to_memory(updated_dir_location)"
      ],
      "metadata": {
        "id": "qiR7LxGoqJt3"
      },
      "id": "qiR7LxGoqJt3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"4\">Organizing According to train-test</font>"
      ],
      "metadata": {
        "id": "dhLUMropr8or"
      },
      "id": "dhLUMropr8or"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get train-test division and parse it."
      ],
      "metadata": {
        "id": "NVTbWWRE6Vo0"
      },
      "id": "NVTbWWRE6Vo0"
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_train_test_txt(url_to_use: str):\n",
        "    url_response = requests.get(url_to_use)\n",
        "    if url_response.status_code == 200:\n",
        "        text_content = url_response.text\n",
        "    else:\n",
        "        raise ValueError(\"Invalid URL\")\n",
        "    ret_text: list[str] = text_content.split('\\n')\n",
        "    examples: list[tuple[tuple[str, int], tuple[str, int], bool]] = list()\n",
        "    for text in ret_text:\n",
        "        separated_by_tabs: list[str] = text.split('\\t')\n",
        "        if len(separated_by_tabs) < 3:\n",
        "            # This is the number in the beginning\n",
        "            continue\n",
        "        if len(separated_by_tabs) == 3:\n",
        "            # This is a positive example (2 Pictures of the same person)\n",
        "            person = separated_by_tabs[0]\n",
        "            first_image_index = int(separated_by_tabs[1])\n",
        "            second_image_index = int(separated_by_tabs[2])\n",
        "            examples.append(\n",
        "                                        (\n",
        "                                             (person, first_image_index),\n",
        "                                             (person, second_image_index),\n",
        "                                             True\n",
        "                                        )\n",
        "                                     )\n",
        "        if len(separated_by_tabs) == 4:\n",
        "            first_person = separated_by_tabs[0]\n",
        "            first_person_image_index = int(separated_by_tabs[1])\n",
        "            second_person = separated_by_tabs[2]\n",
        "            second_person_image_index = int(separated_by_tabs[3])\n",
        "            examples.append(\n",
        "                                        (\n",
        "                                             (first_person, first_person_image_index),\n",
        "                                             (second_person, second_person_image_index),\n",
        "                                             False\n",
        "                                        )\n",
        "                                     )\n",
        "    return examples\n",
        "\n",
        "training_examples = parse_train_test_txt(TRAINING_SET_URL)\n",
        "test_examples = parse_train_test_txt(TEST_SET_URL)"
      ],
      "metadata": {
        "id": "RkEBUAZyvBqR"
      },
      "id": "RkEBUAZyvBqR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(images_file_paths_dict: dict[str, dict[int, str]],\n",
        "                examples_list: list[tuple[tuple[str, int], tuple[str, int], bool]]) -> list[tuple[Image.Image, Image.Image]]:\n",
        "  \"\"\"\n",
        "  Loads the images given to memory in the following format:\n",
        "  Returns 2 lists:\n",
        "  list[loaded_image, loaded_image], list[is_same]\n",
        "  \"\"\"\n",
        "  data_to_ret: list = list()\n",
        "  labels_to_ret: list[bool] = list() # Returned labels, true if same person, false otherwise.\n",
        "  for example in examples_list:\n",
        "    first_person, first_image_index = example[0]\n",
        "    second_person, second_image_index = example[1]\n",
        "    is_same = example[2]\n",
        "    first_image_path = images_file_paths_dict[first_person][first_image_index]\n",
        "    second_image_path = images_file_paths_dict[second_person][second_image_index]\n",
        "    first_image = Image.open(first_image_path)\n",
        "    second_image = Image.open(second_image_path)\n",
        "    if (first_image.mode != IMAGE_MODE) or (second_image.mode != IMAGE_MODE):\n",
        "        raise ValueError(\"Images have different modes.\")\n",
        "    if (first_image.size != IMAGE_SIZE) or (second_image.size != IMAGE_SIZE):\n",
        "        raise ValueError(\"Images have different sizes.\")\n",
        "    data_to_ret.append((first_image, second_image))\n",
        "    labels_to_ret.append(is_same)\n",
        "  return data_to_ret, labels_to_ret\n",
        "\n",
        "training_data, training_labels = load_images(loaded_images, training_examples)\n",
        "test_data, test_labels = load_images(loaded_images, test_examples)"
      ],
      "metadata": {
        "id": "BqvZnIBp6gsl"
      },
      "id": "BqvZnIBp6gsl",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_images_to_array(image_tuple_list: list[tuple[Image.Image, Image.Image]]) -> list[np.ndarray]:\n",
        "  returned_list: list[np.ndarray] = list()\n",
        "  for first_image, second_image in image_tuple_list:\n",
        "    first_image_array = np.array(first_image)\n",
        "    second_image_array = np.array(second_image)\n",
        "    returned_list.append((first_image_array, second_image_array))\n",
        "  return returned_list\n",
        "\n",
        "arrayed_training_data = convert_images_to_array(training_data)\n",
        "arrayed_test_data = convert_images_to_array(test_data)"
      ],
      "metadata": {
        "id": "QWyz_nkdqxT4"
      },
      "id": "QWyz_nkdqxT4",
      "execution_count": 40,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e8da3580c132e0a5"
      },
      "cell_type": "markdown",
      "source": [
        "<font size=\"6\">Handling_Data</font>"
      ],
      "id": "e8da3580c132e0a5"
    },
    {
      "metadata": {
        "id": "c1528f83f48d4ec8"
      },
      "cell_type": "markdown",
      "source": [
        "<font size=\"6\">Creating Network</font>"
      ],
      "id": "c1528f83f48d4ec8"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-24T11:01:51.240312Z",
          "start_time": "2025-04-24T11:01:50.832893Z"
        },
        "id": "bcd78c34b2aa484d"
      },
      "cell_type": "code",
      "source": [
        "# class ConvNet(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.cnn = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=1, out_channels=64, kernel_size=10),  # TODO Change the in channels to support more colors.\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "#             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=7),  # TODO Change the in channels to support more colors.\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "#             nn.Conv2d(in_channels=128, out_channels=128, kernel_size=4),  # TODO Change the in channels to support more colors.\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "#             nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4),  # TODO Change the in channels to support more colors.\n",
        "#             nn.ReLU(),\n",
        "#         )\n",
        "\n",
        "#         self.fc1 = nn.Sequential(\n",
        "#             nn.Flatten(),                         # -> [256*6*6 = 9216]\n",
        "#             nn.Linear(256*6*6, 4096),\n",
        "#             nn.Sigmoid()\n",
        "#         )\n",
        "\n",
        "#         self.fc2 = nn.Sequential(\n",
        "#             nn.Linear(4096, 1),\n",
        "#             nn.Sigmoid()  # similarity score\n",
        "#         )\n",
        "\n",
        "#     def forward_once(self, x):\n",
        "#         x = self.cnn(x)\n",
        "#         x = self.fc1(x)\n",
        "#         return x\n",
        "\n",
        "#     def forward(self, input1, input2):\n",
        "#         out1 = self.forward_once(input1)\n",
        "#         out2 = self.forward_once(input2)\n",
        "#         # L1 distance\n",
        "#         diff = torch.abs(out1 - out2)\n",
        "#         similarity_score = self.fc2(diff)\n",
        "#         return similarity_score\n",
        "\n",
        "\n",
        "\n",
        "# # asd = ConvNet()"
      ],
      "id": "bcd78c34b2aa484d",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}