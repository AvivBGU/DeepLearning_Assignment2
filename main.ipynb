{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvivBGU/DeepLearning_Assignment2/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D39Befbt9pDh",
      "metadata": {
        "id": "D39Befbt9pDh"
      },
      "source": [
        "TODOS:\n",
        "1. Create a proper training setup and embed the stuff implemented in the data loaded to the relevant functions.\n",
        "2. Make sure that loss and iterations, as well as general data is printed.\n",
        "3. Have a testing setup for an arbitrary model.\n",
        "4. Create a model factory to properly balance different parameters of the network.\n",
        "5. Make sure that the testing setup allows the display of images to have visual verifacation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rtJZSgupvMTZ",
      "metadata": {
        "id": "rtJZSgupvMTZ"
      },
      "source": [
        "# Imports & Constants"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UbwrfERBvbOG",
      "metadata": {
        "id": "UbwrfERBvbOG"
      },
      "source": [
        "<font size=\"4\">Imports </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c38be8f",
      "metadata": {},
      "source": [
        "Install Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "037cc722",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbc121e30a2defb3",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2025-04-24T11:01:56.955022Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbc121e30a2defb3",
        "jupyter": {
          "is_executing": true
        },
        "outputId": "a329d461-54d1-4544-e165-a2e99fcc519c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import copy\n",
        "\n",
        "from torchvision import transforms\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "\n",
        "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
        "\n",
        "print(\"Using torch\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "792fda7a90fc3204",
      "metadata": {
        "id": "792fda7a90fc3204"
      },
      "source": [
        "<font size=\"4\">Constants</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ab05b4b7ab2bc28",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-23T12:57:22.636614Z",
          "start_time": "2025-04-23T12:57:22.628140Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ab05b4b7ab2bc28",
        "outputId": "9ae808fd-19fd-4290-e1a6-3cdc722348b9"
      },
      "outputs": [],
      "source": [
        "current_working_directory = os.getcwd()\n",
        "DATA_BASE_DIRECTORY: str = os.path.join(current_working_directory, 'data')\n",
        "TRAINING_SET_URL='https://web.archive.org/web/20241214060505/https://vis-www.cs.umass.edu/lfw/pairsDevTrain.txt'\n",
        "TEST_SET_URL='https://web.archive.org/web/20241214070147/https://vis-www.cs.umass.edu/lfw/pairsDevTest.txt#expand'\n",
        "MAX_PIXEL_VALUE: float = 255.0\n",
        "IMAGE_SIZE: tuple[int, int] = (105, 105)\n",
        "BATCH_SIZE: int = 128\n",
        "IMAGE_MODE: str = 'L' # If the image is greyscale\n",
        "DEVICE_TO_USE: str = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "RANDOM_SEED: int = 10 # For replicatability\n",
        "TRAINING_VALIDATION_DIVISION: float = 0.1\n",
        "MAX_EPOCHS_FOR_TRAINING: int = 200\n",
        "PATIANCE_FACTOR: int = 1e-4\n",
        "ALLOWED_PATIANCE_ITERATIONS: int = 20\n",
        "EARLY_STOP: bool = True\n",
        "print(f'Using device: {DEVICE_TO_USE}')\n",
        "random.seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-2XQCQ27vUIF",
      "metadata": {
        "id": "-2XQCQ27vUIF"
      },
      "source": [
        "# Acquiring & Handling Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6UkO1B1wS3I",
      "metadata": {
        "id": "f6UkO1B1wS3I"
      },
      "source": [
        "<font size=\"6\">Acquiring_Data</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_c2tOa7RwRsW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c2tOa7RwRsW",
        "outputId": "8eb9c076-67eb-48af-81d8-90777783f568"
      },
      "outputs": [],
      "source": [
        "# TODO Refine this mechanism to use python natively.\n",
        "\n",
        "!pip install -q gdown\n",
        "\n",
        "def download_images_from_drive(file_id: str, zip_path: str) -> str:\n",
        "  \"\"\"\n",
        "  Downloads images from drive and return the path to the extracted folder, but 1\n",
        "  level down assuming the structure of the directories are known in advance.\n",
        "  \"\"\"\n",
        "  file_location: str = os.path.join(DATA_BASE_DIRECTORY, 'lfw2', 'lfw2')\n",
        "  if os.path.exists(file_location):\n",
        "    print(f\"Dataset already downloaded to {file_location}\")\n",
        "    return file_location\n",
        "  \n",
        "  !gdown {file_id} -O {zip_path}\n",
        "\n",
        "  os.makedirs(DATA_BASE_DIRECTORY, exist_ok=True)\n",
        "  with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(DATA_BASE_DIRECTORY)\n",
        "  !rm {zip_path}\n",
        "  print(f\"Dataset extracted to {DATA_BASE_DIRECTORY}\")\n",
        "  return file_location\n",
        "\n",
        "updated_dir_location: str = download_images_from_drive(\n",
        "    file_id=\"1p1wjaqpTh_5RHfJu4vUh8JJCdKwYMHCp\",\n",
        "    zip_path=\"dataset.zip\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69YLzhf59ERr",
      "metadata": {
        "id": "69YLzhf59ERr"
      },
      "source": [
        "<font size=\"4\">Preprocessing function</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CzrceAmpqKWD",
      "metadata": {
        "id": "CzrceAmpqKWD"
      },
      "source": [
        "<font size=\"4\">Loading file paths to memory</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qiR7LxGoqJt3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiR7LxGoqJt3",
        "outputId": "225ccba4-4d6f-4cfa-fbce-77a566a77c85"
      },
      "outputs": [],
      "source": [
        "def loads_files_paths_to_memory(base_directory: str, image_format: str = '.jpg') -> None:\n",
        "    images: dict[str, dict[int, str]] = dict()\n",
        "    images_loaded: int = 0\n",
        "    for root, subdirs, files in os.walk(base_directory):\n",
        "        if root == base_directory:\n",
        "            continue\n",
        "        person_name: str = root.split(os.sep)[-1]\n",
        "        if person_name not in images:\n",
        "            images[person_name] = dict()\n",
        "        for file in files:\n",
        "            if not file.endswith(image_format):\n",
        "                raise Warning(f\"File {file} is not a {image_format} file. Continuing...\")\n",
        "                continue\n",
        "            stripped_image: str = file.rstrip(image_format) # File without ending\n",
        "            image_index: int = int(stripped_image.split('_')[-1])\n",
        "            if image_index in images[person_name]:\n",
        "                 raise ValueError(f\"Index: {image_index} collision for: {person_name}\")\n",
        "            images[person_name][image_index] = os.path.join(root, file)\n",
        "            images_loaded += 1\n",
        "    if len(images) < 1:\n",
        "        raise ValueError(f\"No images were found in {base_directory}, aborting...\")\n",
        "    print(f\"People scanned: {len(images)}\")\n",
        "    print(f\"Images loaded: {images_loaded}\")\n",
        "    return images\n",
        "\n",
        "loaded_images: dict[str, dict[str, str]] = loads_files_paths_to_memory(updated_dir_location)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dhLUMropr8or",
      "metadata": {
        "id": "dhLUMropr8or"
      },
      "source": [
        "<font size=\"4\">Organizing According to train-test</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NVTbWWRE6Vo0",
      "metadata": {
        "id": "NVTbWWRE6Vo0"
      },
      "source": [
        "<font size=\"4\">Get train-test division and parse it</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RkEBUAZyvBqR",
      "metadata": {
        "id": "RkEBUAZyvBqR"
      },
      "outputs": [],
      "source": [
        "def parse_train_test_txt(url_to_use: str) -> list[tuple[tuple[str, int], tuple[str, int], bool]]:\n",
        "    url_response = requests.get(url_to_use)\n",
        "    if url_response.status_code == 200:\n",
        "        text_content = url_response.text\n",
        "    else:\n",
        "        raise ValueError(\"Invalid URL\")\n",
        "    ret_text: list[str] = text_content.split('\\n')\n",
        "    examples: list[tuple[tuple[str, int], tuple[str, int], bool]] = list()\n",
        "    for text in ret_text:\n",
        "        separated_by_tabs: list[str] = text.split('\\t')\n",
        "        if len(separated_by_tabs) < 3:\n",
        "            # This is the number in the beginning\n",
        "            continue\n",
        "        if len(separated_by_tabs) == 3:\n",
        "            # This is a positive example (2 Pictures of the same person)\n",
        "            person = separated_by_tabs[0]\n",
        "            first_image_index = int(separated_by_tabs[1])\n",
        "            second_image_index = int(separated_by_tabs[2])\n",
        "            examples.append(\n",
        "                                        (\n",
        "                                             (person, first_image_index),\n",
        "                                             (person, second_image_index),\n",
        "                                             1\n",
        "                                        )\n",
        "                                     )\n",
        "        if len(separated_by_tabs) == 4:\n",
        "            first_person = separated_by_tabs[0]\n",
        "            first_person_image_index = int(separated_by_tabs[1])\n",
        "            second_person = separated_by_tabs[2]\n",
        "            second_person_image_index = int(separated_by_tabs[3])\n",
        "            examples.append(\n",
        "                                        (\n",
        "                                             (first_person, first_person_image_index),\n",
        "                                             (second_person, second_person_image_index),\n",
        "                                             0\n",
        "                                        )\n",
        "                                     )\n",
        "    return examples\n",
        "\n",
        "training_set: list[tuple[tuple[str, int], tuple[str, int], bool]] = parse_train_test_txt(TRAINING_SET_URL)\n",
        "test_set: list[tuple[tuple[str, int], tuple[str, int], bool]] = parse_train_test_txt(TEST_SET_URL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "121d45e2",
      "metadata": {},
      "source": [
        "<font size=\"4\">Getting validation set</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79bd0d27",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO MAKE SURE THE VALIDATION SET TAKES AN EQUAL AMOUNT OF SAME PICTURE AND NOT SAME PICTURE.\n",
        "samples_to_select: int = int(len(training_set)*TRAINING_VALIDATION_DIVISION)\n",
        "validation_set: list[tuple[tuple[str, int], tuple[str, int], bool]] = random.sample(training_set, samples_to_select)\n",
        "training_set: list[tuple[tuple[str, int], tuple[str, int], bool]] = [sample for sample in training_set if sample not in validation_set]\n",
        "print(f'training_set_size: {len(training_set)}')\n",
        "print(f'validation_set_size: {len(validation_set)}')\n",
        "print(f'test_set_size: {len(test_set)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BqvZnIBp6gsl",
      "metadata": {
        "id": "BqvZnIBp6gsl"
      },
      "outputs": [],
      "source": [
        "def load_images(images_file_paths_dict: dict[str, dict[int, str]],\n",
        "                examples_list: list[tuple[tuple[str, int], tuple[str, int], bool]]) -> list[tuple[Image.Image, Image.Image]]:\n",
        "  \"\"\"\n",
        "  Loads the images given to memory in the following format:\n",
        "  Returns 2 lists:\n",
        "  list[loaded_image, loaded_image], list[is_same]\n",
        "  \"\"\"\n",
        "  data_to_ret: list = list()\n",
        "  labels_to_ret: list[bool] = list() # Returned labels, true if same person, false otherwise.\n",
        "  transform = transforms.Compose([ # In case we need/want transform the inputs.\n",
        "      transforms.Resize((105, 105)),\n",
        "      transforms.ToTensor(),\n",
        "  ]) # Important to note, if the input is transformed, then it's normalized.\n",
        "\n",
        "  for example in examples_list:\n",
        "    first_person, first_image_index = example[0]\n",
        "    second_person, second_image_index = example[1]\n",
        "    is_same = example[2]\n",
        "    first_image_path = images_file_paths_dict[first_person][first_image_index]\n",
        "    second_image_path = images_file_paths_dict[second_person][second_image_index]\n",
        "    first_image = Image.open(first_image_path)\n",
        "    second_image = Image.open(second_image_path)\n",
        "    if (first_image.mode != IMAGE_MODE) or (second_image.mode != IMAGE_MODE):\n",
        "      raise ValueError(\"Images have different modes.\")\n",
        "    if (first_image.size != IMAGE_SIZE) and (second_image.size != IMAGE_SIZE):\n",
        "        # Resizing instead of throwing error\n",
        "        first_image = transform(first_image)\n",
        "        second_image = transform(second_image)\n",
        "    data_to_ret.append((first_image, second_image))\n",
        "    labels_to_ret.append(is_same)\n",
        "  return data_to_ret, labels_to_ret\n",
        "\n",
        "training_data, training_labels = load_images(loaded_images, training_set)\n",
        "validation_data, validation_labels = load_images(loaded_images, validation_set)\n",
        "test_data, test_labels = load_images(loaded_images, test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6666f82",
      "metadata": {},
      "source": [
        "<font size=\"4\">Converting images to array to allow the useage of pytorch dataloader</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QWyz_nkdqxT4",
      "metadata": {
        "id": "QWyz_nkdqxT4"
      },
      "outputs": [],
      "source": [
        "def convert_images_to_array(image_tuple_list: list[tuple[Image.Image, Image.Image]]) -> list[np.ndarray]:\n",
        "  returned_list: list[np.ndarray] = list()\n",
        "  for first_image, second_image in image_tuple_list:\n",
        "    arrayed_first_image = np.array(first_image)\n",
        "    arrayed_second_image = np.array(second_image)\n",
        "    normalized_first_image = arrayed_first_image / MAX_PIXEL_VALUE if arrayed_first_image.max() > 1 else arrayed_first_image\n",
        "    normalized_second_image = arrayed_second_image / MAX_PIXEL_VALUE if arrayed_second_image.max() > 1 else arrayed_second_image\n",
        "    returned_list.append((normalized_first_image, normalized_second_image))\n",
        "  return returned_list\n",
        "\n",
        "arrayed_training_data = convert_images_to_array(training_data)\n",
        "arrayed_validation_data = convert_images_to_array(validation_data)\n",
        "arrayed_test_data = convert_images_to_array(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Oc-U7eQzOwjN",
      "metadata": {
        "id": "Oc-U7eQzOwjN"
      },
      "outputs": [],
      "source": [
        "class SiameseDataset(data.Dataset):\n",
        "    def __init__(self, image_pairs: list[tuple], labels: list[int]):\n",
        "        self.image_pairs = image_pairs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1, img2 = self.image_pairs[idx]\n",
        "        img1 = torch.tensor(np.array(img1), dtype=torch.float32)\n",
        "        img2 = torch.tensor(np.array(img2), dtype=torch.float32)\n",
        "        if len(img1.shape) == 2 or len(img2.shape) == 2: # Adding color channel in case original picture didn't have it.\n",
        "            img1 = img1.unsqueeze(0)\n",
        "            img2 = img2.unsqueeze(0)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return img1, img2, label\n",
        "\n",
        "training_dataset: data.Dataset = SiameseDataset(arrayed_training_data, training_labels)\n",
        "validation_dataset: data.Dataset = SiameseDataset(validation_data, validation_labels)\n",
        "test_dataset: data.Dataset = SiameseDataset(arrayed_test_data, test_labels)\n",
        "training_loader: data.DataLoader = torch.utils.data.DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "validation_loader: data.DataLoader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "test_loader: data.DataLoader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1LqVSt-FtCJJ",
      "metadata": {
        "id": "1LqVSt-FtCJJ"
      },
      "source": [
        "# Neural Network definition"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1528f83f48d4ec8",
      "metadata": {
        "id": "c1528f83f48d4ec8"
      },
      "source": [
        "<font size=\"6\">Creating Network</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QqM5UmtSYeRS",
      "metadata": {
        "id": "QqM5UmtSYeRS"
      },
      "outputs": [],
      "source": [
        "class ModularSiameseNetwork(nn.Module):\n",
        "    def __init__(self,\n",
        "                 first_conv_layer: tuple[int, int, int, torch.nn.modules.pooling.MaxPool2d | None],\n",
        "                 other_layers: list[(int, int, torch.nn.modules.pooling.MaxPool2d | None)],\n",
        "                 input_size: list[int, int, int], # channels, height, width\n",
        "                 fully_connected_layer_size: int):\n",
        "        \"\"\"\n",
        "        First_conv_layer: [in_channels, out_channels, kernel_size, should use pooling]\n",
        "        other_layers [(out_channels, kernel_size, should_use_pooling)]\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.main_network_block = nn.Sequential()\n",
        "        first_layer_in_channel, first_layer_out_channel, first_layer_kernel_size, first_layer_use_max_pool = first_conv_layer\n",
        "        self.main_network_block.append(\n",
        "            nn.Conv2d(in_channels=first_layer_in_channel,\n",
        "                      out_channels=first_layer_out_channel,\n",
        "                      kernel_size=first_layer_kernel_size)\n",
        "        )\n",
        "        self.main_network_block.append(\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        if first_layer_use_max_pool:\n",
        "            self.main_network_block.append(\n",
        "                first_layer_use_max_pool\n",
        "            )\n",
        "        prev_layer_output: int = first_layer_out_channel\n",
        "        for current_layer in other_layers:\n",
        "            current_layer_output_channels, current_layer_kernel_size, max_pool = current_layer\n",
        "            self.main_network_block.append(\n",
        "                nn.Conv2d(prev_layer_output,\n",
        "                          current_layer_output_channels,\n",
        "                          kernel_size=current_layer_kernel_size)\n",
        "            )\n",
        "            self.main_network_block.append(\n",
        "                nn.ReLU()\n",
        "            )\n",
        "            if max_pool: # If maxpooling should be used.\n",
        "              self.main_network_block.append(\n",
        "                  max_pool\n",
        "              )\n",
        "            prev_layer_output = current_layer_output_channels\n",
        "\n",
        "        with torch.no_grad(): # Dynamically calculate the size of the layer according to expected input.\n",
        "            dummy = torch.zeros(1, *input_size)\n",
        "            dummy_out = self.main_network_block(dummy)\n",
        "    \n",
        "            flattened_size = dummy_out.view(1, -1).size(1)\n",
        "\n",
        "        self.fully_connected_layer = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(flattened_size, fully_connected_layer_size,),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(fully_connected_layer_size, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # === Initialization ===\n",
        "        with torch.no_grad():\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.Conv2d):\n",
        "                    nn.init.normal_(m.weight, mean=0.0, std=1e-2)\n",
        "                    nn.init.normal_(m.bias, mean=0.5, std=1e-2)\n",
        "                elif isinstance(m, nn.Linear):\n",
        "                    nn.init.normal_(m.weight, mean=0.0, std=1e-2)\n",
        "                    nn.init.normal_(m.bias, mean=0.5, std=2e-1)\n",
        "\n",
        "    def forward_once(self, input):\n",
        "        network_block_output = self.main_network_block(input)\n",
        "        fully_connected_layer_output = self.fully_connected_layer(network_block_output)\n",
        "        return fully_connected_layer_output\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        output1 = self.forward_once(input1)\n",
        "        output2 = self.forward_once(input2)\n",
        "        return self.output_layer(torch.abs(output1 - output2)) # L1 distance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f11270c",
      "metadata": {},
      "source": [
        "<font size=\"6\">Loss & Optimizer</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd78c34b2aa484d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-24T11:01:51.240312Z",
          "start_time": "2025-04-24T11:01:50.832893Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcd78c34b2aa484d",
        "outputId": "0abc141d-98a0-48b3-c70e-312a9cc3e66c"
      },
      "outputs": [],
      "source": [
        "class RegularizedBinaryCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, model, lambda_reg=1e-4):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.bce = nn.BCELoss()\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        loss = self.bce(outputs, targets.float())\n",
        "        reg_loss = sum(torch.sum(p.pow(2)) for p in self.model.parameters() if p.requires_grad and p.ndim > 1).to(DEVICE_TO_USE)\n",
        "        return loss + self.lambda_reg * reg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abbfc4a3",
      "metadata": {},
      "source": [
        "<font size=\"6\">Network initialization</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c1b3f6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "initial_lr = 0.01\n",
        "momentum = 0.5\n",
        "l2_regularition_strength = 1e-4\n",
        "\n",
        "model = ModularSiameseNetwork(\n",
        "    first_conv_layer=(1, 64, 10, nn.MaxPool2d(2)),\n",
        "    other_layers=[(128, 7, nn.MaxPool2d(2)),\n",
        "                  (128, 4, nn.MaxPool2d(2)), \n",
        "                  (256, 4, None)], \n",
        "    input_size=[1, *IMAGE_SIZE],\n",
        "    fully_connected_layer_size=4096\n",
        ")\n",
        "model_in_gpu = model.to(DEVICE_TO_USE)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=initial_lr,             # learning rate\n",
        "    momentum=momentum,       # momentum\n",
        "    weight_decay=l2_regularition_strength  # L2 regularization\n",
        ")\n",
        "# Learning Rate Scheduler Configuration\n",
        "learning_rate_decay_func = lambda epoch: 0.99\n",
        "scheduler = lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=learning_rate_decay_func)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1156ecc",
      "metadata": {},
      "source": [
        "<font size=\"6\">Network training</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64962115",
      "metadata": {},
      "source": [
        "Calculate loss for 1 example after passing it to device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80451667",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_loss(model, loss_criterion, input_1, input_2, targets):\n",
        "    input1_in_device = input_1.to(DEVICE_TO_USE, non_blocking=True)\n",
        "    input2_in_device = input_2.to(DEVICE_TO_USE, non_blocking=True)\n",
        "    labels_in_device = targets.unsqueeze(1).to(DEVICE_TO_USE, non_blocking=True)\n",
        "\n",
        "    output = model(input1_in_device, input2_in_device)\n",
        "    loss = loss_criterion(output, labels_in_device)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4af7425",
      "metadata": {},
      "outputs": [],
      "source": [
        "patiance_for_improvement: int = 0\n",
        "validation_loss: float  = 100000000.0\n",
        "early_stop_triggered: bool = False\n",
        "total_time_start = time.time()\n",
        "for epoch in range(MAX_EPOCHS_FOR_TRAINING):\n",
        "    torch.cuda.empty_cache()\n",
        "    model_in_gpu.train()\n",
        "    start_time = time.time()\n",
        "\n",
        "    if epoch + 1 == 6:  # epoch is 0-indexed\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"momentum\"] = 0.9\n",
        "            \n",
        "    running_loss = 0.0\n",
        "    print(f\"\\nEpoch [{epoch+1}/{MAX_EPOCHS_FOR_TRAINING}]\")\n",
        "    for batch_idx, (input1, input2, targets) in enumerate(training_loader):\n",
        "        if patiance_for_improvement >= ALLOWED_PATIANCE_ITERATIONS and EARLY_STOP:\n",
        "            print(\"Early stopping due to no improvement was triggered.\")\n",
        "            early_stop_triggered = True\n",
        "            break\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss = calculate_loss(model_in_gpu, criterion, input1, input2, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if (batch_idx + 1) % 2 == 0 or (batch_idx + 1) == len(training_loader):\n",
        "            print(f\"  Batch [{batch_idx+1}/{len(training_loader)}], Loss: {loss.item():.4f}\")\n",
        "    model_in_gpu.eval()\n",
        "    with torch.no_grad():\n",
        "        running_validation_loss = 0\n",
        "        for validation_batch_index, (validation_input_1, validation_input_2, validation_targets) in enumerate(validation_loader):\n",
        "            running_validation_loss += calculate_loss(model_in_gpu, criterion, validation_input_1, validation_input_2, validation_targets).item()\n",
        "        running_validation_loss = running_validation_loss / len(validation_loader)\n",
        "        if running_validation_loss - validation_loss < PATIANCE_FACTOR:\n",
        "            validation_loss = running_validation_loss\n",
        "            best_model_state_dict = copy.deepcopy(model_in_gpu.state_dict())\n",
        "            patiance_for_improvement = 0\n",
        "        else:\n",
        "            patiance_for_improvement += 1\n",
        "        print(f\" Validation Set Loss: {running_validation_loss} \")\n",
        "    if early_stop_triggered and EARLY_STOP:\n",
        "        print(\"Ending training early since no validation improvement triggered.\")\n",
        "        break\n",
        "    scheduler.step()\n",
        "    avg_loss = running_loss / len(training_loader)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Epoch [{epoch+1}] completed in {elapsed_time:.2f}s, Average Loss: {avg_loss:.4f}\")\n",
        "model_in_gpu.eval()\n",
        "print(f\"Finished training in: {time.time() - total_time_start}s.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f4f60dd",
      "metadata": {},
      "source": [
        "<font size=\"6\">Evaluation</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3749e8ab",
      "metadata": {},
      "source": [
        "Display an array as a picture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "becdb44c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_data_loaded_to_loader(\n",
        "        test_example_1, \n",
        "        test_example_2, \n",
        "        prediction: float = -1, \n",
        "        expected_result: float = -1):\n",
        "    cpu_input1 = test_example_1.cpu()\n",
        "    cpu_input2 = test_example_2.cpu()\n",
        "\n",
        "    img1_np = cpu_input1[0].squeeze(0).numpy()\n",
        "    img2_np = cpu_input2[0].squeeze(0).numpy()\n",
        "\n",
        "    # Plotting\n",
        "    fig, axs = plt.subplots(1, 2)\n",
        "    axs[0].imshow(img1_np, cmap='gray')\n",
        "    axs[0].set_title('Image 1')\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    axs[1].imshow(img2_np, cmap='gray')\n",
        "    axs[1].set_title('Image 2')\n",
        "    axs[1].axis('off')\n",
        "    plt.show()\n",
        "    if prediction > -1:\n",
        "        print(f\"prediction: {prediction}\")\n",
        "    if expected_result > -1:\n",
        "        print(f\"expected result: {expected_result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "266649c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing\n",
        "if best_model_state_dict is not None:\n",
        "    model_in_gpu.load_state_dict(best_model_state_dict)\n",
        "    model_in_gpu.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dc57ce3",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_in_gpu.eval()\n",
        "with torch.no_grad():\n",
        "    dummy_1 = torch.randn(1, 1, 105, 105).to(DEVICE_TO_USE)\n",
        "    dummy_2 = torch.randn(1, 1, 105, 105).to(DEVICE_TO_USE)\n",
        "    dummy_3 = dummy_1.clone()  # identical to dummy_1\n",
        "\n",
        "    # Should output different results if model is not collapsed\n",
        "    out_same = model_in_gpu(dummy_1, dummy_3)\n",
        "    out_diff = model_in_gpu(dummy_1, dummy_2)\n",
        "\n",
        "    print(\"Output (same):\", out_same.item())\n",
        "    print(\"Output (diff):\", out_diff.item())\n",
        "    print(\"Abs difference:\", abs(out_same.item() - out_diff.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XbgFaAvn9OP5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XbgFaAvn9OP5",
        "outputId": "8c1f29e2-8d8d-45fa-fcf9-dd91d88294d2"
      },
      "outputs": [],
      "source": [
        "test_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "plots_to_open: int = 6\n",
        "iter_for_same = plots_to_open\n",
        "iter_for_different = plots_to_open\n",
        "with torch.no_grad():\n",
        "    for test_input1, test_input2, test_targets in test_loader:\n",
        "        test_input1, test_input2, test_targets = \\\n",
        "        test_input1.to(DEVICE_TO_USE, non_blocking=True), \\\n",
        "        test_input2.to(DEVICE_TO_USE, non_blocking=True), \\\n",
        "        test_targets.unsqueeze(1).to(DEVICE_TO_USE, non_blocking=True)\n",
        "        test_output = model_in_gpu(test_input1, test_input2)\n",
        "        # expected_result = test_targets[0, 0].item()\n",
        "        # if expected_result > 0.9:\n",
        "        #     if iter_for_same > 0:\n",
        "        #         display_data_loaded_to_loader(\n",
        "        #             test_input1,\n",
        "        #             test_input2,\n",
        "        #             prediction=test_output,\n",
        "        #             expected_result=expected_result\n",
        "        #         )\n",
        "        #         iter_for_same -= 1\n",
        "                \n",
        "        # else:\n",
        "        #     if iter_for_different > 0:\n",
        "        #         display_data_loaded_to_loader(\n",
        "        #             test_input1,\n",
        "        #             test_input2,\n",
        "        #             prediction=test_output,\n",
        "        #             expected_result=expected_result\n",
        "        #         )\n",
        "        #         iter_for_different -= 1\n",
        "        batch_loss = criterion(test_output, test_targets)\n",
        "        test_loss += batch_loss.item()\n",
        "        predictions = test_output > 0.5\n",
        "        correct = (predictions.float() == test_targets).sum().item()\n",
        "        correct_predictions += correct\n",
        "        total_predictions += test_targets.size(0)\n",
        "\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "accuracy = correct_predictions / total_predictions\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
