{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvivBGU/DeepLearning_Assignment2/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D39Befbt9pDh",
      "metadata": {
        "id": "D39Befbt9pDh"
      },
      "source": [
        "TODOS:\n",
        "1. Create a proper training setup and embed the stuff implemented in the data loaded to the relevant functions.\n",
        "2. Make sure that loss and iterations, as well as general data is printed.\n",
        "3. Have a testing setup for an arbitrary model.\n",
        "4. Create a model factory to properly balance different parameters of the network.\n",
        "5. Make sure that the testing setup allows the display of images to have visual verifacation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rtJZSgupvMTZ",
      "metadata": {
        "id": "rtJZSgupvMTZ"
      },
      "source": [
        "# Imports & Constants"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UbwrfERBvbOG",
      "metadata": {
        "id": "UbwrfERBvbOG"
      },
      "source": [
        "<font size=\"4\">Imports </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c38be8f",
      "metadata": {},
      "source": [
        "Install Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "037cc722",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbc121e30a2defb3",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2025-04-24T11:01:56.955022Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbc121e30a2defb3",
        "jupyter": {
          "is_executing": true
        },
        "outputId": "a329d461-54d1-4544-e165-a2e99fcc519c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import zipfile\n",
        "import requests\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "\n",
        "print(\"Using torch\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "792fda7a90fc3204",
      "metadata": {
        "id": "792fda7a90fc3204"
      },
      "source": [
        "<font size=\"4\">Constants</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ab05b4b7ab2bc28",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-23T12:57:22.636614Z",
          "start_time": "2025-04-23T12:57:22.628140Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ab05b4b7ab2bc28",
        "outputId": "9ae808fd-19fd-4290-e1a6-3cdc722348b9"
      },
      "outputs": [],
      "source": [
        "current_working_directory = os.getcwd()\n",
        "DATA_BASE_DIRECTORY: str = os.path.join(current_working_directory, 'data')\n",
        "TRAINING_SET_URL='https://web.archive.org/web/20241214060505/https://vis-www.cs.umass.edu/lfw/pairsDevTrain.txt'\n",
        "TEST_SET_URL='https://web.archive.org/web/20241214070147/https://vis-www.cs.umass.edu/lfw/pairsDevTest.txt#expand'\n",
        "MAX_PIXEL_VALUE: float = 255.0\n",
        "IMAGE_SIZE: tuple[int, int] = (250, 250)\n",
        "BATCH_SIZE: int = 8\n",
        "IMAGE_MODE: str = 'L' # If the image is greyscale\n",
        "DEVICE_TO_USE: str = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {DEVICE_TO_USE}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-2XQCQ27vUIF",
      "metadata": {
        "id": "-2XQCQ27vUIF"
      },
      "source": [
        "# Acquiring & Handling Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6UkO1B1wS3I",
      "metadata": {
        "id": "f6UkO1B1wS3I"
      },
      "source": [
        "<font size=\"6\">Acquiring_Data</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_c2tOa7RwRsW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c2tOa7RwRsW",
        "outputId": "8eb9c076-67eb-48af-81d8-90777783f568"
      },
      "outputs": [],
      "source": [
        "# TODO Refine this mechanism to use python natively.\n",
        "\n",
        "!pip install -q gdown\n",
        "\n",
        "def download_images_from_drive(file_id: str, zip_path: str) -> str:\n",
        "  \"\"\"\n",
        "  Downloads images from drive and return the path to the extracted folder, but 1\n",
        "  level down assuming the structure of the directories are known in advance.\n",
        "  \"\"\"\n",
        "  file_location: str = os.path.join(DATA_BASE_DIRECTORY, 'lfw2', 'lfw2')\n",
        "  if os.path.exists(file_location):\n",
        "    print(f\"Dataset already downloaded to {file_location}\")\n",
        "    return file_location\n",
        "  \n",
        "  !gdown {file_id} -O {zip_path}\n",
        "\n",
        "  os.makedirs(DATA_BASE_DIRECTORY, exist_ok=True)\n",
        "  with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(DATA_BASE_DIRECTORY)\n",
        "  !rm {zip_path}\n",
        "  print(f\"Dataset extracted to {DATA_BASE_DIRECTORY}\")\n",
        "  return file_location\n",
        "\n",
        "updated_dir_location: str = download_images_from_drive(\n",
        "    file_id=\"1p1wjaqpTh_5RHfJu4vUh8JJCdKwYMHCp\",\n",
        "    zip_path=\"dataset.zip\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69YLzhf59ERr",
      "metadata": {
        "id": "69YLzhf59ERr"
      },
      "source": [
        "<font size=\"4\">Preprocessing function</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CzrceAmpqKWD",
      "metadata": {
        "id": "CzrceAmpqKWD"
      },
      "source": [
        "<font size=\"4\">Loading file paths to memory</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qiR7LxGoqJt3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiR7LxGoqJt3",
        "outputId": "225ccba4-4d6f-4cfa-fbce-77a566a77c85"
      },
      "outputs": [],
      "source": [
        "def loads_files_paths_to_memory(base_directory: str, image_format: str = '.jpg') -> None:\n",
        "    images: dict[str, dict[int, str]] = dict()\n",
        "    images_loaded: int = 0\n",
        "    for root, subdirs, files in os.walk(base_directory):\n",
        "        if root == base_directory:\n",
        "            continue\n",
        "        person_name: str = root.split(os.sep)[-1]\n",
        "        if person_name not in images:\n",
        "            images[person_name] = dict()\n",
        "        for file in files:\n",
        "            if not file.endswith(image_format):\n",
        "                raise Warning(f\"File {file} is not a {image_format} file. Continuing...\")\n",
        "                continue\n",
        "            stripped_image: str = file.rstrip(image_format) # File without ending\n",
        "            image_index: int = int(stripped_image.split('_')[-1])\n",
        "            if image_index in images[person_name]:\n",
        "                 raise ValueError(f\"Index: {image_index} collision for: {person_name}\")\n",
        "            images[person_name][image_index] = os.path.join(root, file)\n",
        "            images_loaded += 1\n",
        "    if len(images) < 1:\n",
        "        raise ValueError(f\"No images were found in {base_directory}, aborting...\")\n",
        "    print(f\"People scanned: {len(images)}\")\n",
        "    print(f\"Images loaded: {images_loaded}\")\n",
        "    return images\n",
        "\n",
        "loaded_images: dict[str, dict[str, str]] = loads_files_paths_to_memory(updated_dir_location)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dhLUMropr8or",
      "metadata": {
        "id": "dhLUMropr8or"
      },
      "source": [
        "<font size=\"4\">Organizing According to train-test</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NVTbWWRE6Vo0",
      "metadata": {
        "id": "NVTbWWRE6Vo0"
      },
      "source": [
        "Get train-test division and parse it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RkEBUAZyvBqR",
      "metadata": {
        "id": "RkEBUAZyvBqR"
      },
      "outputs": [],
      "source": [
        "def parse_train_test_txt(url_to_use: str):\n",
        "    url_response = requests.get(url_to_use)\n",
        "    if url_response.status_code == 200:\n",
        "        text_content = url_response.text\n",
        "    else:\n",
        "        raise ValueError(\"Invalid URL\")\n",
        "    ret_text: list[str] = text_content.split('\\n')\n",
        "    examples: list[tuple[tuple[str, int], tuple[str, int], bool]] = list()\n",
        "    for text in ret_text:\n",
        "        separated_by_tabs: list[str] = text.split('\\t')\n",
        "        if len(separated_by_tabs) < 3:\n",
        "            # This is the number in the beginning\n",
        "            continue\n",
        "        if len(separated_by_tabs) == 3:\n",
        "            # This is a positive example (2 Pictures of the same person)\n",
        "            person = separated_by_tabs[0]\n",
        "            first_image_index = int(separated_by_tabs[1])\n",
        "            second_image_index = int(separated_by_tabs[2])\n",
        "            examples.append(\n",
        "                                        (\n",
        "                                             (person, first_image_index),\n",
        "                                             (person, second_image_index),\n",
        "                                             1\n",
        "                                        )\n",
        "                                     )\n",
        "        if len(separated_by_tabs) == 4:\n",
        "            first_person = separated_by_tabs[0]\n",
        "            first_person_image_index = int(separated_by_tabs[1])\n",
        "            second_person = separated_by_tabs[2]\n",
        "            second_person_image_index = int(separated_by_tabs[3])\n",
        "            examples.append(\n",
        "                                        (\n",
        "                                             (first_person, first_person_image_index),\n",
        "                                             (second_person, second_person_image_index),\n",
        "                                             0\n",
        "                                        )\n",
        "                                     )\n",
        "    return examples\n",
        "\n",
        "training_examples = parse_train_test_txt(TRAINING_SET_URL)\n",
        "test_examples = parse_train_test_txt(TEST_SET_URL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BqvZnIBp6gsl",
      "metadata": {
        "id": "BqvZnIBp6gsl"
      },
      "outputs": [],
      "source": [
        "def load_images(images_file_paths_dict: dict[str, dict[int, str]],\n",
        "                examples_list: list[tuple[tuple[str, int], tuple[str, int], bool]]) -> list[tuple[Image.Image, Image.Image]]:\n",
        "  \"\"\"\n",
        "  Loads the images given to memory in the following format:\n",
        "  Returns 2 lists:\n",
        "  list[loaded_image, loaded_image], list[is_same]\n",
        "  \"\"\"\n",
        "  data_to_ret: list = list()\n",
        "  labels_to_ret: list[bool] = list() # Returned labels, true if same person, false otherwise.\n",
        "  for example in examples_list:\n",
        "    first_person, first_image_index = example[0]\n",
        "    second_person, second_image_index = example[1]\n",
        "    is_same = example[2]\n",
        "    first_image_path = images_file_paths_dict[first_person][first_image_index]\n",
        "    second_image_path = images_file_paths_dict[second_person][second_image_index]\n",
        "    first_image = Image.open(first_image_path)\n",
        "    second_image = Image.open(second_image_path)\n",
        "    if (first_image.mode != IMAGE_MODE) or (second_image.mode != IMAGE_MODE):\n",
        "        raise ValueError(\"Images have different modes.\")\n",
        "    if (first_image.size != IMAGE_SIZE) or (second_image.size != IMAGE_SIZE):\n",
        "        raise ValueError(\"Images have different sizes.\")\n",
        "    data_to_ret.append((first_image, second_image))\n",
        "    labels_to_ret.append(is_same)\n",
        "  return data_to_ret, labels_to_ret\n",
        "\n",
        "training_data, training_labels = load_images(loaded_images, training_examples)\n",
        "test_data, test_labels = load_images(loaded_images, test_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QWyz_nkdqxT4",
      "metadata": {
        "id": "QWyz_nkdqxT4"
      },
      "outputs": [],
      "source": [
        "def convert_images_to_array(image_tuple_list: list[tuple[Image.Image, Image.Image]]) -> list[np.ndarray]:\n",
        "  returned_list: list[np.ndarray] = list()\n",
        "  for first_image, second_image in image_tuple_list:\n",
        "    normalized_first_image = np.array(first_image) / MAX_PIXEL_VALUE\n",
        "    normalized_second_image = np.array(second_image) / MAX_PIXEL_VALUE\n",
        "    first_image_array = np.array(normalized_first_image)\n",
        "    second_image_array = np.array(normalized_second_image)\n",
        "    returned_list.append((first_image_array, second_image_array))\n",
        "  return returned_list\n",
        "\n",
        "arrayed_training_data = convert_images_to_array(training_data)\n",
        "arrayed_test_data = convert_images_to_array(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Oc-U7eQzOwjN",
      "metadata": {
        "id": "Oc-U7eQzOwjN"
      },
      "outputs": [],
      "source": [
        "class SiameseDataset(data.Dataset):\n",
        "    def __init__(self, image_pairs: list[tuple], labels: list[int]):\n",
        "        self.image_pairs = image_pairs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1, img2 = self.image_pairs[idx]\n",
        "        img1 = torch.tensor(np.array(img1), dtype=torch.float32).unsqueeze(0)  # [1, H, W]\n",
        "        img2 = torch.tensor(np.array(img2), dtype=torch.float32).unsqueeze(0)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return img1, img2, label\n",
        "\n",
        "training_dataset: data.Dataset = SiameseDataset(arrayed_training_data, training_labels)\n",
        "training_loader: data.DataLoader = torch.utils.data.DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "test_dataset: data.Dataset = SiameseDataset(arrayed_test_data, test_labels)\n",
        "test_loader: data.DataLoader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1LqVSt-FtCJJ",
      "metadata": {
        "id": "1LqVSt-FtCJJ"
      },
      "source": [
        "# Neural Network definition and stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1528f83f48d4ec8",
      "metadata": {
        "id": "c1528f83f48d4ec8"
      },
      "source": [
        "<font size=\"6\">Creating Network</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QqM5UmtSYeRS",
      "metadata": {
        "id": "QqM5UmtSYeRS"
      },
      "outputs": [],
      "source": [
        "class ModularSiameseNetwork(nn.Module):\n",
        "    def __init__(self,\n",
        "                 first_conv_layer: tuple[int, int, int, bool],\n",
        "                 other_layers: list[int, int, bool],\n",
        "                 input_size: list[int, int], # height, width, assumes 1 channel.\n",
        "                 fully_connected_layer_size: int):\n",
        "        super().__init__()\n",
        "        self.main_network_block = nn.Sequential()\n",
        "        first_layer_in_channel, first_layer_out_channel, first_layer_kernel_size, first_layer_use_max_pool = first_conv_layer\n",
        "        self.main_network_block.append(\n",
        "            nn.Conv2d(in_channels=first_layer_in_channel,\n",
        "                      out_channels=first_layer_out_channel,\n",
        "                      kernel_size=first_layer_kernel_size)\n",
        "        )\n",
        "        self.main_network_block.append(\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        if first_layer_use_max_pool:\n",
        "            self.main_network_block.append(\n",
        "                nn.MaxPool2d(2)\n",
        "            )\n",
        "        prev_layer_output: int = first_layer_out_channel\n",
        "        for current_layer in other_layers:\n",
        "            current_layer_output_channels, current_layer_kernel_size, use_max_pool = current_layer\n",
        "            self.main_network_block.append(\n",
        "                nn.Conv2d(prev_layer_output,\n",
        "                          current_layer_output_channels,\n",
        "                          kernel_size=current_layer_kernel_size)\n",
        "            )\n",
        "            self.main_network_block.append(\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "            if use_max_pool: # If maxpooling should be used.\n",
        "              self.main_network_block.append(\n",
        "                  nn.MaxPool2d(2)\n",
        "              )\n",
        "            prev_layer_output = current_layer_output_channels\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, 1, *input_size)\n",
        "            dummy_out = self.main_network_block(dummy)\n",
        "            flattened_size = dummy_out.view(1, -1).size(1)\n",
        "\n",
        "        self.fully_connected_layer = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(flattened_size, fully_connected_layer_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(fully_connected_layer_size, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward_once(self, input):\n",
        "        network_block_output = self.main_network_block(input)\n",
        "        fully_connected_layer_output = self.fully_connected_layer(network_block_output)\n",
        "        return fully_connected_layer_output\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        output1 = self.forward_once(input1)\n",
        "        output2 = self.forward_once(input2)\n",
        "        return self.output_layer(torch.abs(output1 - output2))\n",
        "\n",
        "ModularSiameseNetwork(\n",
        "    first_conv_layer=(1, 64, 10, True),\n",
        "    other_layers=[(128, 7, True), (128, 4, True), (256, 4, False)],\n",
        "    input_size=[105, 105],\n",
        "    fully_connected_layer_size=4096\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd78c34b2aa484d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-24T11:01:51.240312Z",
          "start_time": "2025-04-24T11:01:50.832893Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcd78c34b2aa484d",
        "outputId": "0abc141d-98a0-48b3-c70e-312a9cc3e66c"
      },
      "outputs": [],
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=10),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=7),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=4),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "        dummy_input = torch.zeros(1, 1, 250, 250)\n",
        "        dummy_output = self.cnn(dummy_input)\n",
        "        flattened_size = dummy_output.view(1, -1).size(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(flattened_size, 4096),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.final_fc = nn.Linear(4096, 1)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight, mean=0.0, std=1e-2)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.normal_(m.bias, mean=0.5, std=1e-2)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, mean=0.0, std=2e-1)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.normal_(m.bias, mean=0.5, std=1e-2)\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        output1 = self.forward_once(input1)\n",
        "        output2 = self.forward_once(input2)\n",
        "\n",
        "        l1_distance = torch.abs(output1 - output2)\n",
        "        output = self.final_fc(l1_distance)\n",
        "        return output\n",
        "\n",
        "class RegularizedBinaryCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, model, lambda_reg=1e-4):\n",
        "        super(RegularizedBinaryCrossEntropyLoss, self).__init__()\n",
        "        self.model = model\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        loss = self.bce(outputs, targets)\n",
        "        reg_loss = 0.0\n",
        "        for param in self.model.parameters():\n",
        "            reg_loss += torch.sum(param ** 2)\n",
        "        total_loss = loss + self.lambda_reg * reg_loss\n",
        "        return total_loss\n",
        "\n",
        "class CustomSGDWithMomentum(torch.optim.Optimizer):\n",
        "    def __init__(self, params, lr=0.01, momentum=0.9, weight_decay=1e-4):\n",
        "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "        super(CustomSGDWithMomentum, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                if group['weight_decay'] != 0:\n",
        "                    d_p.add_(2 * group['weight_decay'], p.data)\n",
        "                if 'momentum_buffer' not in self.state[p]:\n",
        "                    buf = self.state[p]['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                else:\n",
        "                    buf = self.state[p]['momentum_buffer']\n",
        "                    buf.mul_(group['momentum']).add_(d_p, alpha=-group['lr'])\n",
        "                p.data.add_(buf)\n",
        "\n",
        "        return loss\n",
        "\n",
        "model = SiameseNetwork().to(DEVICE_TO_USE)\n",
        "criterion = RegularizedBinaryCrossEntropyLoss(model, lambda_reg=1e-4)\n",
        "optimizer = CustomSGDWithMomentum(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for input1, input2, targets in training_loader:\n",
        "        input1, input2, targets = input1.to(DEVICE_TO_USE, non_blocking=True),\n",
        "        input2.to(DEVICE_TO_USE, non_blocking=True),\n",
        "        targets.unsqueeze(1).to(DEVICE_TO_USE, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input1, input2)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XbgFaAvn9OP5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XbgFaAvn9OP5",
        "outputId": "8c1f29e2-8d8d-45fa-fcf9-dd91d88294d2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Testing\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "plots_to_open: int = 6\n",
        "iter_for_same = plots_to_open\n",
        "iter_for_different = plots_to_open\n",
        "with torch.no_grad():\n",
        "    for test_input1, test_input2, test_targets in test_loader:\n",
        "        test_input1, test_input2, test_targets = \\\n",
        "        test_input1.to(DEVICE_TO_USE, non_blocking=True), \\\n",
        "        test_input2.to(DEVICE_TO_USE, non_blocking=True), \\\n",
        "        test_targets.unsqueeze(1).to(DEVICE_TO_USE, non_blocking=True)\n",
        "\n",
        "\n",
        "        test_output = model(test_input1, test_input2)\n",
        "\n",
        "        expected_result = test_targets[0, 0].item()\n",
        "\n",
        "\n",
        "        if expected_result > 0.9:\n",
        "            if iter_for_same > 0:\n",
        "\n",
        "                cpu_input1 = test_input1.cpu()\n",
        "                cpu_input2 = test_input2.cpu()\n",
        "\n",
        "                img1_np = cpu_input1[0].squeeze(0).numpy()\n",
        "                img2_np = cpu_input2[0].squeeze(0).numpy()\n",
        "\n",
        "                # Plotting\n",
        "                fig, axs = plt.subplots(1, 2)\n",
        "                axs[0].imshow(img1_np, cmap='gray')\n",
        "                axs[0].set_title('Image 1')\n",
        "                axs[0].axis('off')\n",
        "\n",
        "                axs[1].imshow(img2_np, cmap='gray')\n",
        "                axs[1].set_title('Image 2')\n",
        "                axs[1].axis('off')\n",
        "                iter_for_same -= 1\n",
        "                plt.show()\n",
        "                print(f\"prediction: {test_output}\")\n",
        "                print(f\"expected result: {expected_result}\")\n",
        "        else:\n",
        "            if iter_for_different > 0:\n",
        "\n",
        "                cpu_input1 = test_input1.cpu()\n",
        "                cpu_input2 = test_input2.cpu()\n",
        "\n",
        "                img1_np = cpu_input1[0].squeeze(0).numpy()\n",
        "                img2_np = cpu_input2[0].squeeze(0).numpy()\n",
        "\n",
        "                # Plotting\n",
        "                fig, axs = plt.subplots(1, 2)\n",
        "                axs[0].imshow(img1_np, cmap='gray')\n",
        "                axs[0].set_title('Image 1')\n",
        "                axs[0].axis('off')\n",
        "\n",
        "                axs[1].imshow(img2_np, cmap='gray')\n",
        "                axs[1].set_title('Image 2')\n",
        "                axs[1].axis('off')\n",
        "                iter_for_different -= 1\n",
        "                plt.show()\n",
        "                print(f\"prediction: {test_output}\")\n",
        "                print(f\"expected result: {expected_result}\")\n",
        "\n",
        "        batch_loss = criterion(test_output, test_targets)\n",
        "        test_loss += batch_loss.item()\n",
        "        predictions = torch.sigmoid(test_output) > 0.5\n",
        "        correct = (predictions.float() == test_targets).sum().item()\n",
        "        correct_predictions += correct\n",
        "        total_predictions += test_targets.size(0)\n",
        "\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "accuracy = correct_predictions / total_predictions\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
