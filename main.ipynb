{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvivBGU/DeepLearning_Assignment2/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rtJZSgupvMTZ",
      "metadata": {
        "id": "rtJZSgupvMTZ"
      },
      "source": [
        "# Imports & Constants"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UbwrfERBvbOG",
      "metadata": {
        "id": "UbwrfERBvbOG"
      },
      "source": [
        "<font size=\"4\">Imports </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c38be8f",
      "metadata": {},
      "source": [
        "Install Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "037cc722",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
        "!pip3 install torch torchsummary\n",
        "!pip3 install -q gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbc121e30a2defb3",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2025-04-24T11:01:56.955022Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbc121e30a2defb3",
        "jupyter": {
          "is_executing": true
        },
        "outputId": "a329d461-54d1-4544-e165-a2e99fcc519c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import copy\n",
        "import gdown\n",
        "from torchsummary import summary\n",
        "from collections import defaultdict\n",
        "from torchvision import transforms\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "\n",
        "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
        "\n",
        "print(\"Using torch\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "792fda7a90fc3204",
      "metadata": {
        "id": "792fda7a90fc3204"
      },
      "source": [
        "<font size=\"4\">Constants</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ab05b4b7ab2bc28",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-23T12:57:22.636614Z",
          "start_time": "2025-04-23T12:57:22.628140Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ab05b4b7ab2bc28",
        "outputId": "9ae808fd-19fd-4290-e1a6-3cdc722348b9"
      },
      "outputs": [],
      "source": [
        "current_working_directory = os.getcwd()\n",
        "DATA_BASE_DIRECTORY: str = os.path.join(current_working_directory, 'data')\n",
        "TRAIN_TEST_DIVISION_LOCATION = os.path.join(current_working_directory, 'training_set_division')\n",
        "TRAINING_SET_URL='https://web.archive.org/web/20241214060505/https://vis-www.cs.umass.edu/lfw/pairsDevTrain.txt'\n",
        "TEST_SET_URL='https://web.archive.org/web/20241214070147/https://vis-www.cs.umass.edu/lfw/pairsDevTest.txt#expand'\n",
        "MAX_PIXEL_VALUE: float = 255.0\n",
        "IMAGE_SIZE: tuple[int, int] = (105, 105)\n",
        "BATCH_SIZE: int = 128\n",
        "IMAGE_MODE: str = 'L' # If the image is greyscale\n",
        "DEVICE_TO_USE: str = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "RANDOM_SEED: int = 10 # For replicatability\n",
        "TRAINING_VALIDATION_DIVISION: float = 0.1\n",
        "MAX_EPOCHS_FOR_TRAINING: int = 200\n",
        "PATIANCE_FACTOR: int = 1e-4\n",
        "ALLOWED_PATIANCE_ITERATIONS: int = 5\n",
        "EARLY_STOP: bool = True\n",
        "DEFAULT_LR = 1e-2\n",
        "DEFAULT_MOMEMENTUM = 0.5\n",
        "DEFAULT_L2_REGULARIZATION_STRENGTH = 1e-4\n",
        "DEFAULT_LEARNING_RATE_DECAY_FUNC = lambda epoch: 0.99\n",
        "\n",
        "print(f'Using device: {DEVICE_TO_USE}')\n",
        "random.seed(RANDOM_SEED) # Using a set seed to achieve reproducibility\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-2XQCQ27vUIF",
      "metadata": {
        "id": "-2XQCQ27vUIF"
      },
      "source": [
        "# Acquiring & Handling Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6UkO1B1wS3I",
      "metadata": {
        "id": "f6UkO1B1wS3I"
      },
      "source": [
        "<font size=\"6\">Acquiring_Data</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_c2tOa7RwRsW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c2tOa7RwRsW",
        "outputId": "8eb9c076-67eb-48af-81d8-90777783f568"
      },
      "outputs": [],
      "source": [
        "def download_images_from_drive(file_id: str, download_to_path: str, extract_zip_to: str = DATA_BASE_DIRECTORY) -> str:\n",
        "  \"\"\"\n",
        "  Downloads images from drive and return the path to the extracted folder, but 1\n",
        "  level down assuming the structure of the directories are known in advance.\n",
        "  \"\"\"\n",
        "  file_location: str = os.path.join(extract_zip_to, 'lfw2', 'lfw2')\n",
        "  if os.path.exists(file_location):\n",
        "    print(f\"Dataset already downloaded to {file_location}\")\n",
        "    return file_location\n",
        "  \n",
        "  downloaded_zip_name: str = gdown.download(id='1p1wjaqpTh_5RHfJu4vUh8JJCdKwYMHCp', output=download_to_path)\n",
        "  print(downloaded_zip_name)\n",
        "  os.makedirs(extract_zip_to, exist_ok=True)\n",
        "  with zipfile.ZipFile(downloaded_zip_name, 'r') as zip_ref:\n",
        "      zip_ref.extractall(extract_zip_to)\n",
        "  # os.remove(downloaded_zip_name)\n",
        "  print(f\"Dataset extracted to {extract_zip_to}\")\n",
        "  return file_location\n",
        "\n",
        "updated_dir_location: str = download_images_from_drive(\n",
        "    file_id=\"1p1wjaqpTh_5RHfJu4vUh8JJCdKwYMHCp\",\n",
        "    download_to_path=os.path.join(os.getcwd(), 'dataset.zip')\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69YLzhf59ERr",
      "metadata": {
        "id": "69YLzhf59ERr"
      },
      "source": [
        "<font size=\"4\">Preprocessing function</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CzrceAmpqKWD",
      "metadata": {
        "id": "CzrceAmpqKWD"
      },
      "source": [
        "<font size=\"4\">Loading file paths to memory</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qiR7LxGoqJt3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiR7LxGoqJt3",
        "outputId": "225ccba4-4d6f-4cfa-fbce-77a566a77c85"
      },
      "outputs": [],
      "source": [
        "def loads_files_paths_to_memory(base_directory: str, image_format: str = '.jpg') -> None:\n",
        "    images: dict[str, dict[int, str]] = dict()\n",
        "    images_loaded: int = 0\n",
        "    for root, subdirs, files in os.walk(base_directory):\n",
        "        if root == base_directory:\n",
        "            continue\n",
        "        person_name: str = root.split(os.sep)[-1]\n",
        "        if person_name not in images:\n",
        "            images[person_name] = dict()\n",
        "        for file in files:\n",
        "            if not file.endswith(image_format):\n",
        "                raise Warning(f\"File {file} is not a {image_format} file. Continuing...\")\n",
        "            stripped_image: str = file.rstrip(image_format) # File without ending\n",
        "            image_index: int = int(stripped_image.split('_')[-1])\n",
        "            if image_index in images[person_name]:\n",
        "                 raise ValueError(f\"Index: {image_index} collision for: {person_name}\")\n",
        "            images[person_name][image_index] = os.path.join(root, file)\n",
        "            images_loaded += 1\n",
        "    if len(images) < 1:\n",
        "        raise ValueError(f\"No images were found in {base_directory}, aborting...\")\n",
        "    print(f\"People scanned: {len(images)}\")\n",
        "    print(f\"Images loaded: {images_loaded}\")\n",
        "    return images\n",
        "\n",
        "loaded_images: dict[str, dict[str, str]] = loads_files_paths_to_memory(updated_dir_location)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dhLUMropr8or",
      "metadata": {
        "id": "dhLUMropr8or"
      },
      "source": [
        "<font size=\"4\">Organizing According to train-test</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NVTbWWRE6Vo0",
      "metadata": {
        "id": "NVTbWWRE6Vo0"
      },
      "source": [
        "<font size=\"4\">Get train-test division and parse it</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e73ba49",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_train_test_division(url_to_use: str, save_location: str, file_name: str) -> str:\n",
        "    \"\"\"\n",
        "    Gets the train test division from the wayback machine and saves it locally.\n",
        "    \"\"\"\n",
        "    full_file_location: str = os.path.join(save_location, file_name)\n",
        "    if os.path.exists(full_file_location):\n",
        "        \"\"\"\n",
        "        Already downloaded.\n",
        "        \"\"\"\n",
        "        print(f'{full_file_location} was already downloaded, continuing...')\n",
        "        return full_file_location\n",
        "    os.makedirs(save_location, exist_ok=True) # Creates the dirs listed in the save location, no error if they already exist.\n",
        "    url_response = requests.get(url_to_use)\n",
        "    if url_response.status_code == 200:\n",
        "        text_content = url_response.text\n",
        "    else:\n",
        "        raise ValueError(\"Cannot get train test division\")\n",
        "    with open(full_file_location, \"w\") as file:\n",
        "        file.write(text_content)\n",
        "    return full_file_location\n",
        "\n",
        "training_data_location: str = get_train_test_division(TRAINING_SET_URL, TRAIN_TEST_DIVISION_LOCATION, \"training_file.tsv\")\n",
        "test_data_location: str = get_train_test_division(TEST_SET_URL, TRAIN_TEST_DIVISION_LOCATION, 'test_file.tsv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RkEBUAZyvBqR",
      "metadata": {
        "id": "RkEBUAZyvBqR"
      },
      "outputs": [],
      "source": [
        "def parse_train_test_txt(data_location: str) -> list[tuple[tuple[str, int], tuple[str, int], bool]]:\n",
        "    if not os.path.exists(data_location):\n",
        "        raise ValueError(f\"Data division not found in: {data_location}.\")\n",
        "    with open(data_location, \"r\") as data:\n",
        "        read_data = data.read()\n",
        "    ret_text: list[str] = read_data.split('\\n')\n",
        "    examples: list[tuple[tuple[str, int], tuple[str, int], bool]] = list()\n",
        "    for text in ret_text:\n",
        "        separated_by_tabs: list[str] = text.split('\\t')\n",
        "        if len(separated_by_tabs) < 3:\n",
        "            # This is the number in the beginning\n",
        "            continue\n",
        "        if len(separated_by_tabs) == 3:\n",
        "            # This is a positive example (2 Pictures of the same person)\n",
        "            person = separated_by_tabs[0]\n",
        "            first_image_index = int(separated_by_tabs[1])\n",
        "            second_image_index = int(separated_by_tabs[2])\n",
        "            examples.append(\n",
        "                                        (\n",
        "                                             (person, first_image_index),\n",
        "                                             (person, second_image_index),\n",
        "                                             1\n",
        "                                        )\n",
        "                                     )\n",
        "        if len(separated_by_tabs) == 4:\n",
        "            first_person = separated_by_tabs[0]\n",
        "            first_person_image_index = int(separated_by_tabs[1])\n",
        "            second_person = separated_by_tabs[2]\n",
        "            second_person_image_index = int(separated_by_tabs[3])\n",
        "            examples.append(\n",
        "                                        (\n",
        "                                             (first_person, first_person_image_index),\n",
        "                                             (second_person, second_person_image_index),\n",
        "                                             0\n",
        "                                        )\n",
        "                                     )\n",
        "    return examples\n",
        "\n",
        "training_set: list[tuple[tuple[str, int], tuple[str, int], bool]] = parse_train_test_txt(training_data_location)\n",
        "test_set: list[tuple[tuple[str, int], tuple[str, int], bool]] = parse_train_test_txt(test_data_location)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eb69d50",
      "metadata": {},
      "source": [
        "<font size=\"4\">Divide training set to positive and negative examples to achieve correct division in validation set</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edfb3a41",
      "metadata": {},
      "outputs": [],
      "source": [
        "positive_training_examples: list[tuple[tuple[str, int], tuple[str, int], bool]] = [training_example for training_example in training_set if training_example[2] == 1]\n",
        "negative_training_examples: list[tuple[tuple[str, int], tuple[str, int], bool]] = [training_example for training_example in training_set if training_example[2] == 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b6cf104",
      "metadata": {},
      "source": [
        "<font size=\"5\">Analyzing the datasets</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "676efdc2",
      "metadata": {},
      "source": [
        "<font size=\"4\">Auxilliary Functions for analysis</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec9856b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_number_of_unique_examples(set_of_pairs: list[tuple[tuple[str, int], tuple[str, int], bool]]) -> dict:\n",
        "    unique_people_dict: dict = defaultdict(set)\n",
        "    for first_person, second_person, _ in set_of_pairs:\n",
        "        first_person_name, first_person_image_index = first_person\n",
        "        second_person_name, second_person_image_index = second_person\n",
        "        unique_people_dict[first_person_name].add(first_person_image_index)\n",
        "        unique_people_dict[second_person_name].add(second_person_image_index)\n",
        "    return unique_people_dict\n",
        "\n",
        "\n",
        "def divide_numbers_to_buckets(list_of_ints: list[int], list_of_buckets: list[int]) -> tuple[dict[int, int], list, list]:\n",
        "    dictionary_for_buckets: dict = defaultdict(lambda: 0)\n",
        "    min_value_in_bucket_list: int = min(list_of_buckets)\n",
        "    max_value_in_bucket_list: int = max(list_of_buckets)\n",
        "    overshooting_list: list = list()\n",
        "    undershooting_list: list = list()\n",
        "    for number in list_of_ints:\n",
        "        if number > max_value_in_bucket_list:\n",
        "            overshooting_list.append(number)\n",
        "        if number < min_value_in_bucket_list:\n",
        "            undershooting_list.append(number)\n",
        "        dictionary_for_buckets[number] += 1\n",
        "    return dict(dictionary_for_buckets), overshooting_list, undershooting_list\n",
        "    \n",
        "        \n",
        "def print_data_regarding_dataset(dataset: list[tuple[tuple[str, int], tuple[str, int], bool]], name_of_dataset: str):\n",
        "    positive_training_examples: list[tuple[tuple[str, int], tuple[str, int], bool]] = [example for example in dataset if example[2] == 1]\n",
        "    negative_training_examples: list[tuple[tuple[str, int], tuple[str, int], bool]] = [example for example in dataset if example[2] == 0]\n",
        "    print(f\"Number of pair in {name_of_dataset}: {len(dataset)}\")\n",
        "    print(f\"Number of positive pairs in {name_of_dataset}: {len(positive_training_examples)}\")\n",
        "    print(f\"Number of negative pairs in {name_of_dataset}: {len(negative_training_examples)}\")\n",
        "    unique_people_mapped: dict = calculate_number_of_unique_examples(dataset)\n",
        "    number_of_examples_per_class: list = [len(number_of_images_used_set) for number_of_images_used_set in unique_people_mapped.values()]\n",
        "    average_number_of_examples_per_class: float = sum(number_of_examples_per_class)/len(unique_people_mapped.keys())\n",
        "    min_examples_for_class = min(number_of_examples_per_class)\n",
        "    max_examples_for_class = max(number_of_examples_per_class)\n",
        "    number_of_examples_divided_to_buckets, overshooting_examples, undershooting_examples = divide_numbers_to_buckets(number_of_examples_per_class, list_of_buckets=[_ for _ in range(1, 9)])\n",
        "    print(f\"Number of unique classes in {name_of_dataset}: {len(unique_people_mapped.keys())}\")\n",
        "    print(f\"Average number of unique examples per class in {name_of_dataset}: {average_number_of_examples_per_class}\")\n",
        "    print(f\"Min number of examples per class in {name_of_dataset}: {min_examples_for_class}\")\n",
        "    print(f\"Max number of examples per class in {name_of_dataset}: {max_examples_for_class}\")\n",
        "    print(f\"Number of examples for each class divided to buckets in {name_of_dataset}: {dict(sorted(number_of_examples_divided_to_buckets.items()))}\")\n",
        "    print(f\"Number of overshooting outliers in {name_of_dataset}: {overshooting_examples}\")\n",
        "    print(f\"Number of undershooting outliers in {name_of_dataset}: {undershooting_examples}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc51877f",
      "metadata": {},
      "source": [
        "<font size=\"4\">Analyzing Training Set</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6681adc3",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Printing training set prior to dividing it to validation and testing.\")\n",
        "print_data_regarding_dataset(training_set, \"Training set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "121d45e2",
      "metadata": {},
      "source": [
        "<font size=\"4\">Getting validation set in such a way to make sure that it contains the same number of positive and negative examples.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79bd0d27",
      "metadata": {},
      "outputs": [],
      "source": [
        "samples_to_select: int = int(len(training_set)*TRAINING_VALIDATION_DIVISION)\n",
        "# Making sure the validation set contains an equal amount of positive and negative examples.\n",
        "validation_set: list[tuple[tuple[str, int], tuple[str, int], bool]] = random.sample(positive_training_examples, int(samples_to_select/2))\n",
        "validation_set.extend(random.sample(negative_training_examples, int(samples_to_select/2)))\n",
        "training_set: list[tuple[tuple[str, int], tuple[str, int], bool]] = [sample for sample in training_set if sample not in validation_set]\n",
        "print(f'training_set_size: {len(training_set)}')\n",
        "print(f'validation_set_size: {len(validation_set)}')\n",
        "print(f'test_set_size: {len(test_set)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1c415f5",
      "metadata": {},
      "source": [
        "<font size=\"4\">Analyzing Training Set</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9af5bbbb",
      "metadata": {},
      "outputs": [],
      "source": [
        "print_data_regarding_dataset(training_set, \"Training Set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae3a5397",
      "metadata": {},
      "source": [
        "<font size=\"4\">Analyzing Validation Set</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cec9d99",
      "metadata": {},
      "outputs": [],
      "source": [
        "print_data_regarding_dataset(validation_set, \"Validation Set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21e5e1a4",
      "metadata": {},
      "source": [
        "<font size=\"4\">Analyzing Test Set</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf8da188",
      "metadata": {},
      "outputs": [],
      "source": [
        "print_data_regarding_dataset(test_set, \"Test Set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4f352a1",
      "metadata": {},
      "source": [
        "<font size=\"5\">Loading images to datasets</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BqvZnIBp6gsl",
      "metadata": {
        "id": "BqvZnIBp6gsl"
      },
      "outputs": [],
      "source": [
        "def load_images(images_file_paths_dict: dict[str, dict[int, str]],\n",
        "                examples_list: list[tuple[tuple[str, int], tuple[str, int], bool]]) -> list[tuple[Image.Image, Image.Image]]:\n",
        "  \"\"\"\n",
        "  Loads the images given to memory in the following format:\n",
        "  Returns 2 lists:\n",
        "  list[loaded_image, loaded_image], list[is_same]\n",
        "  \"\"\"\n",
        "  data_to_ret: list = list()\n",
        "  labels_to_ret: list[bool] = list() # Returned labels, true if same person, false otherwise.\n",
        "  transform = transforms.Compose([ # In case we need/want transform the inputs.\n",
        "      transforms.Resize(IMAGE_SIZE),\n",
        "      transforms.ToTensor(),\n",
        "  ]) # Important to note, if the input is transformed, then it's normalized.\n",
        "\n",
        "  for example in examples_list:\n",
        "    first_person, first_image_index = example[0]\n",
        "    second_person, second_image_index = example[1]\n",
        "    is_same = example[2]\n",
        "    first_image_path = images_file_paths_dict[first_person][first_image_index]\n",
        "    second_image_path = images_file_paths_dict[second_person][second_image_index]\n",
        "    first_image = Image.open(first_image_path)\n",
        "    second_image = Image.open(second_image_path)\n",
        "    if (first_image.mode != IMAGE_MODE) or (second_image.mode != IMAGE_MODE):\n",
        "      raise ValueError(\"Images have different modes.\")\n",
        "    if (first_image.size != IMAGE_SIZE) and (second_image.size != IMAGE_SIZE):\n",
        "        # Resizing instead of throwing error\n",
        "        first_image = transform(first_image)\n",
        "        second_image = transform(second_image)\n",
        "    data_to_ret.append((first_image, second_image))\n",
        "    labels_to_ret.append(is_same)\n",
        "  return data_to_ret, labels_to_ret\n",
        "\n",
        "training_data, training_labels = load_images(loaded_images, training_set)\n",
        "validation_data, validation_labels = load_images(loaded_images, validation_set)\n",
        "test_data, test_labels = load_images(loaded_images, test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6666f82",
      "metadata": {},
      "source": [
        "<font size=\"4\">Converting images to array to allow the useage of pytorch dataloader</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QWyz_nkdqxT4",
      "metadata": {
        "id": "QWyz_nkdqxT4"
      },
      "outputs": [],
      "source": [
        "def convert_images_to_array(image_tuple_list: list[tuple[Image.Image, Image.Image]]) -> list[np.ndarray]:\n",
        "  returned_list: list[np.ndarray] = list()\n",
        "  for first_image, second_image in image_tuple_list:\n",
        "    arrayed_first_image = np.array(first_image)\n",
        "    arrayed_second_image = np.array(second_image)\n",
        "    normalized_first_image = arrayed_first_image / MAX_PIXEL_VALUE if arrayed_first_image.max() > 1 else arrayed_first_image\n",
        "    normalized_second_image = arrayed_second_image / MAX_PIXEL_VALUE if arrayed_second_image.max() > 1 else arrayed_second_image\n",
        "    returned_list.append((normalized_first_image, normalized_second_image))\n",
        "  return returned_list\n",
        "\n",
        "arrayed_training_data = convert_images_to_array(training_data)\n",
        "arrayed_validation_data = convert_images_to_array(validation_data)\n",
        "arrayed_test_data = convert_images_to_array(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Oc-U7eQzOwjN",
      "metadata": {
        "id": "Oc-U7eQzOwjN"
      },
      "outputs": [],
      "source": [
        "class SiameseDataset(data.Dataset):\n",
        "    def __init__(self, image_pairs: list[tuple], labels: list[int]):\n",
        "        self.image_pairs = image_pairs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1, img2 = self.image_pairs[idx]\n",
        "        img1 = torch.tensor(np.array(img1), dtype=torch.float32)\n",
        "        img2 = torch.tensor(np.array(img2), dtype=torch.float32)\n",
        "        if len(img1.shape) == 2 or len(img2.shape) == 2: # Adding color channel in case original picture didn't have it.\n",
        "            img1 = img1.unsqueeze(0)\n",
        "            img2 = img2.unsqueeze(0)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return img1, img2, label\n",
        "\n",
        "training_dataset: data.Dataset = SiameseDataset(arrayed_training_data, training_labels)\n",
        "validation_dataset: data.Dataset = SiameseDataset(validation_data, validation_labels)\n",
        "test_dataset: data.Dataset = SiameseDataset(arrayed_test_data, test_labels)\n",
        "training_loader: data.DataLoader = torch.utils.data.DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "validation_loader: data.DataLoader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "test_loader: data.DataLoader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1LqVSt-FtCJJ",
      "metadata": {
        "id": "1LqVSt-FtCJJ"
      },
      "source": [
        "# Neural Network definition"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1528f83f48d4ec8",
      "metadata": {
        "id": "c1528f83f48d4ec8"
      },
      "source": [
        "<font size=\"6\">Creating Network</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QqM5UmtSYeRS",
      "metadata": {
        "id": "QqM5UmtSYeRS"
      },
      "outputs": [],
      "source": [
        "class ModularSiameseNetwork(nn.Module):\n",
        "    def __init__(self,\n",
        "                 first_conv_layer: tuple[int, int, int, torch.nn.modules.pooling.MaxPool2d | None],\n",
        "                 other_layers: list[(int, int, torch.nn.modules.pooling.MaxPool2d | None)],\n",
        "                 input_size: list[int, int, int], # channels, height, width\n",
        "                 fully_connected_layer_size: int,\n",
        "                 apply_batchnorm: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.main_network_block = nn.Sequential()\n",
        "        first_layer_in_channel, first_layer_out_channel, first_layer_kernel_size, first_layer_use_max_pool = first_conv_layer\n",
        "        self.main_network_block.append(\n",
        "            nn.Conv2d(in_channels=first_layer_in_channel,\n",
        "                      out_channels=first_layer_out_channel,\n",
        "                      kernel_size=first_layer_kernel_size)\n",
        "        )\n",
        "        if apply_batchnorm:\n",
        "            self.main_network_block.append(\n",
        "                nn.BatchNorm2d(first_layer_out_channel)\n",
        "            )\n",
        "        self.main_network_block.append(\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        if first_layer_use_max_pool:\n",
        "            self.main_network_block.append(\n",
        "                first_layer_use_max_pool\n",
        "            )\n",
        "        prev_layer_output: int = first_layer_out_channel\n",
        "        for current_layer in other_layers:\n",
        "            current_layer_output_channels, current_layer_kernel_size, max_pool = current_layer\n",
        "            self.main_network_block.append(\n",
        "                nn.Conv2d(prev_layer_output,\n",
        "                          current_layer_output_channels,\n",
        "                          kernel_size=current_layer_kernel_size)\n",
        "            )\n",
        "            if apply_batchnorm:\n",
        "                self.main_network_block.append(\n",
        "                    nn.BatchNorm2d(current_layer_output_channels)\n",
        "                )\n",
        "            self.main_network_block.append(\n",
        "                nn.ReLU()\n",
        "            )\n",
        "            if max_pool: # If maxpooling should be used.\n",
        "              self.main_network_block.append(\n",
        "                  max_pool\n",
        "              )\n",
        "            prev_layer_output = current_layer_output_channels\n",
        "\n",
        "        with torch.no_grad(): # Dynamically calculate the size of the layer according to expected input.\n",
        "            dummy = torch.zeros(1, *input_size)\n",
        "            dummy_out = self.main_network_block(dummy)\n",
        "    \n",
        "            flattened_size = dummy_out.view(1, -1).size(1)\n",
        "\n",
        "        self.fully_connected_layer = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(flattened_size, fully_connected_layer_size,),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(fully_connected_layer_size, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # === Initialization ===\n",
        "        with torch.no_grad():\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.Conv2d):\n",
        "                    nn.init.normal_(m.weight, mean=0.0, std=1e-2)\n",
        "                    nn.init.normal_(m.bias, mean=0.5, std=1e-2)\n",
        "                elif isinstance(m, nn.Linear):\n",
        "                    nn.init.normal_(m.weight, mean=0.0, std=1e-2)\n",
        "                    nn.init.normal_(m.bias, mean=0.5, std=2e-1)\n",
        "\n",
        "    def forward_once(self, input):\n",
        "        network_block_output = self.main_network_block(input)\n",
        "        fully_connected_layer_output = self.fully_connected_layer(network_block_output)\n",
        "        return fully_connected_layer_output\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        output1 = self.forward_once(input1)\n",
        "        output2 = self.forward_once(input2)\n",
        "        return self.output_layer(torch.abs(output1 - output2)) # L1 distance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bc19074",
      "metadata": {},
      "source": [
        "A class designed to encapsulate all the data and variables requires to run a training set as well as use intelligent scheduling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c366992e",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SiameseNetworkWithScheduler:\n",
        "    def __init__(self, network: ModularSiameseNetwork, \n",
        "                 criterion: str,\n",
        "                 optimizer_to_use: str = \"SGD\",\n",
        "                 optimizer_params: dict | None = None,\n",
        "                 scheduler_params: dict | None = None,\n",
        "                 use_scheduler: bool = True,\n",
        "                 print_summary: bool = True\n",
        "                 ):\n",
        "        self.inner_network = network.to(DEVICE_TO_USE)\n",
        "        if criterion == \"BCE\":\n",
        "            self.criterion = nn.BCELoss()\n",
        "        else:\n",
        "            raise ValueError(f\"Criterion: {criterion} not supported\")\n",
        "        if print_summary:\n",
        "            print(f'Optimizer to use: {optimizer_to_use}')\n",
        "            print(f'Criterion to use: {criterion}')\n",
        "            print(f'Initial_LR: {optimizer_params.get(\"initial_lr\", DEFAULT_LR)}')\n",
        "            print(f'L2_Regularization_Strength: {optimizer_params.get(\"l2_regularition_strength\", DEFAULT_L2_REGULARIZATION_STRENGTH)}')\n",
        "            summary(self.inner_network.main_network_block, (1, *IMAGE_SIZE), batch_size=128)\n",
        "        if not optimizer_params:\n",
        "            optimizer_params = dict()\n",
        "        if not scheduler_params:\n",
        "            scheduler_params = dict()\n",
        "        initial_lr = optimizer_params.get(\"initial_lr\", DEFAULT_LR)\n",
        "        l2_regularition_strength = optimizer_params.get(\"l2_regularition_strength\", DEFAULT_L2_REGULARIZATION_STRENGTH)\n",
        "        self.optimizer_as_str = optimizer_to_use\n",
        "        if optimizer_to_use == \"SGD\":\n",
        "            momentum = optimizer_params.get(\"momentum\", DEFAULT_MOMEMENTUM)\n",
        "            optimizer = torch.optim.SGD(\n",
        "                network.parameters(),\n",
        "                lr=initial_lr,             # learning rate\n",
        "                momentum=momentum,       # momentum\n",
        "                weight_decay=l2_regularition_strength  # L2 regularization\n",
        "            )\n",
        "        elif optimizer_to_use == \"ADAM\":\n",
        "            optimizer = torch.optim.Adam(\n",
        "                network.parameters(),\n",
        "                lr=initial_lr,             # learning rate\n",
        "                weight_decay=l2_regularition_strength  # L2 regularization\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Optimizer: {optimizer_to_use} not supported\")\n",
        "        self.optimizer = optimizer\n",
        "        if use_scheduler == True:\n",
        "            learning_rate_decay_func = scheduler_params.get(\"learning_rate_decay_func\", DEFAULT_LEARNING_RATE_DECAY_FUNC)\n",
        "            self.inner_scheduler = lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=learning_rate_decay_func)\n",
        "        else:\n",
        "            self.inner_scheduler = lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda epoch: 1) # Does nothing, by design to make the flow more coherent.\n",
        "        self.inner_network.eval() # Making sure it's not mutated accidently.\n",
        "\n",
        "    @classmethod\n",
        "    def calculate_loss(cls, model, loss_criterion, input_1, input_2, targets):\n",
        "        # Calculate loss for 1 example after passing it to device\n",
        "        input1_in_device = input_1.to(DEVICE_TO_USE, non_blocking=True)\n",
        "        input2_in_device = input_2.to(DEVICE_TO_USE, non_blocking=True)\n",
        "        labels_in_device = targets.unsqueeze(1).to(DEVICE_TO_USE, non_blocking=True)\n",
        "        output = model(input1_in_device, input2_in_device)\n",
        "        loss = loss_criterion(output, labels_in_device)\n",
        "        return loss\n",
        "\n",
        "    def train(self,\n",
        "              training_loader: data.DataLoader, \n",
        "              validation_loader: data.DataLoader, \n",
        "              use_early_stopping: bool = EARLY_STOP,\n",
        "              patiance_factor: float = PATIANCE_FACTOR,\n",
        "              patiance_in_epochs: int = ALLOWED_PATIANCE_ITERATIONS,\n",
        "              max_epochs: int = MAX_EPOCHS_FOR_TRAINING\n",
        "              ):\n",
        "        training_loss_per_epoch: list[int] = list()\n",
        "        validation_loss_per_epoch: list[int] = list()\n",
        "        patiance_for_improvement: int = -1\n",
        "        validation_loss: float  = 100000000.0\n",
        "        early_stop_triggered: bool = False\n",
        "        performance_degraded: bool = False\n",
        "        best_model_state_dict: dict | None = None\n",
        "        total_time_start = time.time()\n",
        "        with torch.no_grad():\n",
        "            running_validation_loss = 0 # Loss without any training\n",
        "            for validation_input_1, validation_input_2, validation_targets in validation_loader:\n",
        "                running_validation_loss += self.calculate_loss(self.inner_network, self.criterion, validation_input_1, validation_input_2, validation_targets).item()\n",
        "            running_validation_loss = running_validation_loss / len(validation_loader)\n",
        "            validation_loss_per_epoch.append(running_validation_loss)\n",
        "            running_loss = 0.0\n",
        "            for input1, input2, targets in training_loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                loss = self.calculate_loss(self.inner_network, self.criterion, input1, input2, targets)\n",
        "                running_loss += loss.item()\n",
        "            avg_loss = running_loss / len(training_loader)\n",
        "            training_loss_per_epoch.append(avg_loss)\n",
        "        for epoch in range(max_epochs):\n",
        "            if early_stop_triggered: \n",
        "                break\n",
        "            self.inner_network.train()\n",
        "            start_time = time.time()\n",
        "            running_loss = 0.0\n",
        "            for input1, input2, targets in training_loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                loss = self.calculate_loss(self.inner_network, self.criterion, input1, input2, targets)\n",
        "                running_loss += loss.item()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "            if use_early_stopping:\n",
        "                with torch.no_grad():\n",
        "                    running_validation_loss = 0\n",
        "                    for validation_input_1, validation_input_2, validation_targets in validation_loader:\n",
        "                        running_validation_loss += self.calculate_loss(self.inner_network, self.criterion, validation_input_1, validation_input_2, validation_targets).item()\n",
        "                    running_validation_loss = running_validation_loss / len(validation_loader)\n",
        "                    validation_loss_per_epoch.append(running_validation_loss)\n",
        "                    if running_validation_loss - validation_loss < patiance_factor:\n",
        "                        validation_loss = running_validation_loss\n",
        "                        best_model_state_dict = copy.deepcopy(self.inner_network.state_dict())\n",
        "                        patiance_for_improvement = -1\n",
        "                        performance_degraded = False\n",
        "                    else:\n",
        "                        patiance_for_improvement += 1\n",
        "                        performance_degraded = True\n",
        "                    if patiance_for_improvement >= patiance_in_epochs and use_early_stopping:\n",
        "                        print(\"Early stopping due to no improvement was triggered.\")\n",
        "                        early_stop_triggered = True\n",
        "            self.inner_scheduler.step()\n",
        "            avg_loss = running_loss / len(training_loader)\n",
        "            training_loss_per_epoch.append(avg_loss)\n",
        "            elapsed_time = time.time() - start_time\n",
        "            self.inner_network.eval()\n",
        "            \n",
        "            print(f\"Epoch [{epoch+1}/{max_epochs}] completed in {elapsed_time:.2f}s, Average Training Loss: {avg_loss:.4f}, Average Validation Loss: {running_validation_loss}\")\n",
        "            if performance_degraded:\n",
        "                print(f\"No significant improvement detected (At least {patiance_factor}). Patiance factor:{patiance_for_improvement + 1}/{patiance_in_epochs}\")\n",
        "            if early_stop_triggered and use_early_stopping:\n",
        "                print(\"Ending training early since no validation improvement triggered.\")\n",
        "                break\n",
        "\n",
        "        # Loads the best model in the previous step since the model's performance might've degraded since the patiance factor.\n",
        "        if best_model_state_dict:\n",
        "            self.inner_network.load_state_dict(best_model_state_dict)\n",
        "            self.inner_network = self.inner_network.to(DEVICE_TO_USE)\n",
        "        self.inner_network.eval()\n",
        "        print(f\"Finished training in: {time.time() - total_time_start}s.\")\n",
        "        return training_loss_per_epoch, validation_loss_per_epoch\n",
        "    def eval(self, input1, input2):\n",
        "        self.inner_network.eval()\n",
        "        with torch.no_grad():\n",
        "            return self.inner_network(\n",
        "                input1.to(DEVICE_TO_USE, non_blocking=True),\n",
        "                input2.to(DEVICE_TO_USE, non_blocking=True)\n",
        "            )\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e82e320a",
      "metadata": {},
      "source": [
        "<font size=\"4\">A function meant to test the model by running the test data.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87fe8d51",
      "metadata": {},
      "outputs": [],
      "source": [
        "def testing_the_model(\n",
        "        model_to_test: SiameseNetworkWithScheduler,\n",
        "        test_loader: data.DataLoader,\n",
        "        number_of_samples_to_save: int = 5\n",
        "        ) -> tuple[list, list]:\n",
        "    test_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    correct_positive_examples: list = list()\n",
        "    correct_negative_examples: list = list()\n",
        "    incorrect_positive_examples: list = list()\n",
        "    incorrect_negative_examples: list = list()\n",
        "    positive_examples: list = (correct_positive_examples, correct_negative_examples)\n",
        "    negative_examples: list = (incorrect_positive_examples, incorrect_negative_examples)\n",
        "    with torch.no_grad():\n",
        "        for test_input1, test_input2, test_target in test_loader:\n",
        "               test_input1, test_input2, test_target = \\\n",
        "               test_input1.to(DEVICE_TO_USE, non_blocking=True), \\\n",
        "               test_input2.to(DEVICE_TO_USE, non_blocking=True), \\\n",
        "               test_target.unsqueeze(1).to(DEVICE_TO_USE, non_blocking=True)\n",
        "               same_face: bool = True if test_target.sum() >= 0.5 else False\n",
        "               test_output = model_to_test.eval(test_input1, test_input2)\n",
        "               batch_loss = model_to_test.criterion(test_output, test_target)\n",
        "               test_loss += batch_loss.item()\n",
        "               test_output_as_float: float = test_output.sum()\n",
        "               predictions = test_output > 0.5\n",
        "               correct = (predictions.float() == test_target).sum().item()\n",
        "               append_to: list | None = None\n",
        "               if correct:\n",
        "                   if same_face:\n",
        "                        if len(correct_positive_examples) < number_of_samples_to_save:\n",
        "                             append_to = correct_positive_examples\n",
        "                   else:\n",
        "                        if len(correct_negative_examples) < number_of_samples_to_save:\n",
        "                             append_to = correct_negative_examples\n",
        "               else:\n",
        "                   if same_face:\n",
        "                        if len(incorrect_positive_examples) < number_of_samples_to_save:\n",
        "                             append_to = incorrect_positive_examples\n",
        "                   else:\n",
        "                        if len(incorrect_negative_examples) < number_of_samples_to_save:\n",
        "                             append_to = incorrect_negative_examples\n",
        "               if append_to is not None:\n",
        "                    append_to.append((test_input1, test_input2, test_output_as_float))\n",
        "               correct_predictions += correct\n",
        "               total_predictions += test_target.size(0)\n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "    return positive_examples, negative_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b41ac9b",
      "metadata": {},
      "source": [
        "Display an array as a picture. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5349b72",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_data_loaded_to_loader(\n",
        "        test_example_1, \n",
        "        test_example_2, \n",
        "        prediction: float = -1, \n",
        "        expected_result: float = -1):\n",
        "    cpu_input1 = test_example_1.cpu()\n",
        "    cpu_input2 = test_example_2.cpu()\n",
        "\n",
        "    img1_np = cpu_input1[0].squeeze(0).numpy()\n",
        "    img2_np = cpu_input2[0].squeeze(0).numpy()\n",
        "\n",
        "    # Plotting\n",
        "    fig, axs = plt.subplots(1, 2)\n",
        "    axs[0].imshow(img1_np, cmap='gray')\n",
        "    axs[0].set_title('Image 1')\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    axs[1].imshow(img2_np, cmap='gray')\n",
        "    axs[1].set_title('Image 2')\n",
        "    axs[1].axis('off')\n",
        "    text = f\"Prediction: {prediction:.3f}, \"\n",
        "    text += f\"Ground Truth: {expected_result:.3f}\"\n",
        "\n",
        "    fig.suptitle(text, fontsize=14, y=0.95)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08f4d82e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_graphs_for_loss(training_loss_per_epoch: list[float], validation_loss_per_epoch: list[float]):\n",
        "    plt.title(\"Training and validation loss over epochs\")\n",
        "    plt.plot(training_loss_per_epoch[:], label=\"Training loss\")\n",
        "    plt.plot(validation_loss_per_epoch[:], label=\"Validation loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abbfc4a3",
      "metadata": {},
      "source": [
        "<font size=\"6\">Architecture Testing</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2298232f",
      "metadata": {},
      "source": [
        "<font size=\"5\">Testing and evaluating SGD optimizer with batchnorm</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30316161",
      "metadata": {},
      "source": [
        "<font size=\"4\">Initializing</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c1b3f6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "if IMAGE_SIZE == (105, 105):\n",
        "    model = ModularSiameseNetwork(\n",
        "        first_conv_layer=(1, 64, 10, nn.MaxPool2d(2)),\n",
        "        other_layers=[(128, 7, nn.MaxPool2d(2)),\n",
        "                    (128, 4, nn.MaxPool2d(2)), \n",
        "                    (256, 4, None)], \n",
        "        input_size=[1, *IMAGE_SIZE],\n",
        "        fully_connected_layer_size=4096,\n",
        "        apply_batchnorm=True\n",
        "    )\n",
        "elif IMAGE_SIZE == (175, 175):\n",
        "    model = ModularSiameseNetwork(\n",
        "        first_conv_layer=(1, 64, 20, nn.MaxPool2d(2)),\n",
        "        other_layers=[(64, 10, nn.MaxPool2d(2)),\n",
        "                    (128, 7, nn.MaxPool2d(2)),\n",
        "                    (128, 4, nn.MaxPool2d(2)), \n",
        "                    (256, 4, None)], \n",
        "        input_size=[1, *IMAGE_SIZE],\n",
        "        fully_connected_layer_size=4096,\n",
        "        apply_batchnorm=True\n",
        "    )\n",
        "\n",
        "complete_sgd_network = SiameseNetworkWithScheduler(\n",
        "    network=model,\n",
        "    criterion=\"BCE\",\n",
        "    optimizer_to_use=\"SGD\",\n",
        "    optimizer_params={'initial_lr': 1e-2}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1156ecc",
      "metadata": {},
      "source": [
        "<font size=\"4\">Training</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4af7425",
      "metadata": {},
      "outputs": [],
      "source": [
        "training_loss_per_epoch, validation_loss_per_epoch = complete_sgd_network.train(\n",
        "    training_loader=training_loader,\n",
        "    validation_loader=validation_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f4f60dd",
      "metadata": {},
      "source": [
        "<font size=\"4\">Evaluation</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XbgFaAvn9OP5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XbgFaAvn9OP5",
        "outputId": "8c1f29e2-8d8d-45fa-fcf9-dd91d88294d2"
      },
      "outputs": [],
      "source": [
        "positive_examples, negative_examples = testing_the_model(complete_sgd_network, test_loader, number_of_samples_to_save=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeccc0ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "display_graphs_for_loss(training_loss_per_epoch, validation_loss_per_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45031bc1",
      "metadata": {},
      "outputs": [],
      "source": [
        "for input1, input2, prediction in positive_examples[0]:\n",
        "    display_data_loaded_to_loader(input1, input2, prediction, expected_result=1)\n",
        "for input1, input2, prediction in positive_examples[1]:\n",
        "    display_data_loaded_to_loader(input1, input2, prediction, expected_result=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1eca593",
      "metadata": {},
      "outputs": [],
      "source": [
        "for input1, input2, prediction in negative_examples[0]:\n",
        "    display_data_loaded_to_loader(input1, input2, prediction, expected_result=1)\n",
        "for input1, input2, prediction in negative_examples[1]:\n",
        "    display_data_loaded_to_loader(input1, input2, prediction, expected_result=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f837ba32",
      "metadata": {},
      "source": [
        "<font size=\"5\">Testing and evaluating ADAM optimizer with batchnorm</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a630c5f3",
      "metadata": {},
      "source": [
        "<font size=\"4\">Initializing</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad7c670f",
      "metadata": {},
      "outputs": [],
      "source": [
        "if IMAGE_SIZE == (105, 105):\n",
        "    model = ModularSiameseNetwork(\n",
        "        first_conv_layer=(1, 64, 10, nn.MaxPool2d(2)),\n",
        "        other_layers=[(128, 7, nn.MaxPool2d(2)),\n",
        "                    (128, 4, nn.MaxPool2d(2)), \n",
        "                    (256, 4, None)], \n",
        "        input_size=[1, *IMAGE_SIZE],\n",
        "        fully_connected_layer_size=4096,\n",
        "        apply_batchnorm=True\n",
        "    )\n",
        "elif IMAGE_SIZE == (175, 175):\n",
        "    model = ModularSiameseNetwork(\n",
        "        first_conv_layer=(1, 64, 20, nn.MaxPool2d(2)),\n",
        "        other_layers=[(64, 10, nn.MaxPool2d(2)),\n",
        "                    (128, 7, nn.MaxPool2d(2)),\n",
        "                    (128, 4, nn.MaxPool2d(2)), \n",
        "                    (256, 4, None)], \n",
        "        input_size=[1, *IMAGE_SIZE],\n",
        "        fully_connected_layer_size=4096,\n",
        "        apply_batchnorm=True\n",
        "    )\n",
        "complete_adam_network = SiameseNetworkWithScheduler(\n",
        "    network=model,\n",
        "    criterion=\"BCE\",\n",
        "    optimizer_to_use=\"ADAM\",\n",
        "    optimizer_params={'initial_lr': 1e-4}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "901e6395",
      "metadata": {},
      "source": [
        "<font size=\"4\">Training</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21feb3d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "training_loss_per_epoch, validation_loss_per_epoch = complete_adam_network.train(\n",
        "    training_loader=training_loader,\n",
        "    validation_loader=validation_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "917e0f89",
      "metadata": {},
      "source": [
        "<font size=\"4\">Evaluation</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "139fca9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "positive_examples, negative_examples = testing_the_model(complete_adam_network, test_loader, number_of_samples_to_save=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bebc8cb2",
      "metadata": {},
      "outputs": [],
      "source": [
        "display_graphs_for_loss(training_loss_per_epoch, validation_loss_per_epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dcc0633",
      "metadata": {},
      "outputs": [],
      "source": [
        "for input1, input2, prediction in positive_examples[0]:\n",
        "    display_data_loaded_to_loader(input1, input2, prediction, expected_result=1)\n",
        "for input1, input2, prediction in positive_examples[1]:\n",
        "    display_data_loaded_to_loader(input1, input2, prediction, expected_result=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68dbe414",
      "metadata": {},
      "outputs": [],
      "source": [
        "for input1, input2, prediction in negative_examples[0]:\n",
        "    display_data_loaded_to_loader(input1, input2, prediction, expected_result=1)\n",
        "for input1, input2, prediction in negative_examples[1]:\n",
        "    display_data_loaded_to_loader(input1, input2, prediction, expected_result=0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
